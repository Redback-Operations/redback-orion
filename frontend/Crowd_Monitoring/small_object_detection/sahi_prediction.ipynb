{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/milieureka/redback-orion/blob/main/Crowd_Monitoring/small_object_detection/sahi_prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fvM4Q0VOmBJE"
      },
      "source": [
        "# Detecting Small Objects with SAHI\n",
        "\n",
        "**Project Name**: Small object dection using SAHI, visualize on FifftyOne platform\n",
        "\n",
        "**Author**: Miley Nguyen  \n",
        "\n",
        "**Team**: RedBack - Crowd Mornitoring\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TUYIxS4TmBJH"
      },
      "source": [
        "Object detection is one of the fundamental tasks in computer vision, but detecting small objects can be particularly challenging.\n",
        "\n",
        "I'll apply SAHI [Slicing Aided Hyper Inference](https://ieeexplore.ieee.org/document/9897990) with Ultralytics' YOLOv8 model to detect small objects in a crowd human images, and then evaluate these predictions to better understand how slicing impacts detection performance.\n",
        "\n",
        "This notebook covers the following:\n",
        "\n",
        "- Loading the VisDrone dataset from the Hugging Face Hub\n",
        "- Applying Ultralytics' YOLOv8 model to the images and video\n",
        "- Using SAHI to run inference on slices of the images and video\n",
        "- Evaluating model performance with and without SAHI (compare with the groud truth label)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tv0QEkEWmBJH"
      },
      "source": [
        "## Setup and Installation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6xHlI6UcmBJI"
      },
      "source": [
        "Dependencies:\n",
        "\n",
        "-  `Python 3.10.12`\n",
        "- `fiftyone` for dataset exploration and manipulation.\n",
        "- `huggingface_hub` Python library for accessing models and datasets.\n",
        "- `ultralytics` official package for running YOLOv8 models, including inference and training.\n",
        "- `sahi` for slicing aided hyper inference.\n",
        "- `IPython` interactive shell capabilities, displaying rich media like videos.\n",
        "- `opencv-python` (cv2) reading and manipulating video and image frames.\n",
        "- `os` â€“ for file management tasks.\n",
        "- [`video`](https://github.com/milieureka/redback-orion/blob/main/Crowd_Monitoring/small_object_detection/resources/Open%20Day%20at%20Deakin%20University%20(online-video-cutter.com).mp4) sample for inference on video. In my code, i download code and upload to Google drive directory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZykP1b22mBJI",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "pip install -U fiftyone sahi ultralytics huggingface_hub --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-dEKJCHgmBJK"
      },
      "outputs": [],
      "source": [
        "import fiftyone as fo\n",
        "import fiftyone.zoo as foz\n",
        "import fiftyone.utils.huggingface as fouh\n",
        "from fiftyone import ViewField as F\n",
        "from ultralytics import YOLO\n",
        "from IPython.display import Video, display, YouTubeVideo\n",
        "import os\n",
        "import cv2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r2PRSQY_mBJK"
      },
      "source": [
        "I use available [VisDrone](https://github.com/VisDrone/VisDrone-Dataset) dataset, this aldready been annotated, more convinient for for the evaluation.\n",
        "The dataset can be accessed in the FiftyOne Hugging Face hub."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gWWZ-512mBJK"
      },
      "outputs": [],
      "source": [
        "# load a subset of VisDrone dataset directly from the Hugging Face Hub\n",
        "dataset = fouh.load_from_hub(\"Voxel51/VisDrone2019-DET\", name=\"sahi-test\", max_samples=100, overwrite=True)\n",
        "dataset_view = dataset.take(50, 50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lAv1_Hj_mBJK"
      },
      "source": [
        "Before adding any predictions, I launch the data to FiftyOne App which is a graphical user interface that makes it easy to explore and rapidly gain intuition into the datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZhKiiZkMmBJK",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "session = fo.launch_app(dataset_view)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eyxLdOV1mBJL"
      },
      "source": [
        "![VisDrone](https://github.com/milieureka/redback-orion/blob/main/Crowd_Monitoring/small_object_detection/resources/Fiftyone%20app.png?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rr5cEPHImBJL"
      },
      "source": [
        "## Standard Inference with YOLOv8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6BkjajS3mBJM"
      },
      "outputs": [],
      "source": [
        "# Load YOLOv8 model from FiftyOne model integration with Ultralytics\n",
        "model = foz.load_zoo_model(\"yolov8l-coco-torch\")\n",
        "ckpt_path = model.config.model_path\n",
        "\n",
        "# Apply the model to the dataset for prediction\n",
        "dataset.apply_model(model, label_field=\"base_model\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lkIB7nV0mBJM",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# Visualize prediction on FiftyOne app\n",
        "session = fo.launch_app(dataset_view)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1wIeEEgemBJM"
      },
      "source": [
        "![Base Model Predictions](https://github.com/milieureka/redback-orion/blob/main/Crowd_Monitoring/small_object_detection/resources/yolov8_predict.gif?raw=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7YQJxi4amBJN"
      },
      "source": [
        "While the model does a pretty good job of detecting objects, it struggles with the small objects, especially people in the distance. This can happen with large images, as most detection models are trained on fixed-size images. As an example, YOLOv8 is trained on images with maximum side length $640$. When we feed it an image of size $1920$ x $1080$, the model will downsample the image to $640$ x $360$ before making predictions. This downsampling can cause small objects to be missed, as the model may not have enough information to detect them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_-t85FJjmBJM"
      },
      "source": [
        "For evaluation, we need to standardize the class labels. This is because the classes detected by our *YOLOv8l* model differ from those in the VisDrone dataset. The YOLO model was trained on the [COCO dataset](https://docs.voxel51.com/user_guide/dataset_zoo/datasets.html#coco-2017), which contains 80 classes, while the VisDrone dataset includes only 12 classes, along with an `ignore_regions` class. To ensure consistency, we will remove the unmatched classes between the two datasets and map the VisDrone classes to their corresponding COCO classes as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UCD5cTromBJM"
      },
      "outputs": [],
      "source": [
        "# Map common classes to the dataset\n",
        "mapping = {\"pedestrians\": \"person\", \"people\": \"person\", \"van\": \"car\"}\n",
        "mapped_view = dataset.map_labels(\"ground_truth\", mapping)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9OmcOo6UmBJN"
      },
      "outputs": [],
      "source": [
        "# Define fuction to filter labels from VisDrone to only include the classes that are in common:\n",
        "\n",
        "def get_label_fields(sample_collection):\n",
        "    label_fields = list(\n",
        "        sample_collection.get_field_schema(embedded_doc_type=fo.Detections).keys()\n",
        "    )\n",
        "    return label_fields\n",
        "\n",
        "def filter_all_labels(sample_collection):\n",
        "    label_fields = get_label_fields(sample_collection)\n",
        "\n",
        "    filtered_view = sample_collection\n",
        "\n",
        "    for lf in label_fields:\n",
        "        filtered_view = filtered_view.filter_labels(\n",
        "            lf, F(\"label\").is_in([\"person\", \"car\", \"truck\"]), only_matches=False\n",
        "        )\n",
        "    return filtered_view"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EwOG6ek6mBJN"
      },
      "outputs": [],
      "source": [
        "filtered_view = filter_all_labels(mapped_view).take(50, 50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xxyskYElmBJN",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "session.view = filtered_view.view()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l545rHsXmBJN"
      },
      "source": [
        "## Detecting Small Objects with SAHI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JQkAscUomBJO"
      },
      "source": [
        "Theoretically, one could train a model on larger images to improve detection of small objects, but this would require more memory and computational power. Another option is to introduce a sliding window approach, where we split the image into smaller patches, run the model on each patch, and then combine the results. This is the idea behind [Slicing Aided Hyper Inference](https://github.com/obss/sahi) (SAHI)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KIMU6mXkmBJO"
      },
      "source": [
        "<figure>\n",
        "  <img src=\"https://raw.githubusercontent.com/obss/sahi/main/resources/sliced_inference.gif\" alt=\"Alt text\" style=\"width:100%\">\n",
        "  <figcaption style=\"text-align:center; color:gray;\">Illustration of Slicing Aided Hyper Inference. Image courtesy of SAHI Github Repo.</figcaption>\n",
        "</figure>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rmI6-uN2mBJO"
      },
      "outputs": [],
      "source": [
        "# Import detection model from sahi framework\n",
        "from sahi import AutoDetectionModel\n",
        "from sahi.predict import get_prediction, get_sliced_prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JEbvw5demBJO"
      },
      "outputs": [],
      "source": [
        "# Define model and define instances and classes\n",
        "detection_model = AutoDetectionModel.from_pretrained(\n",
        "    model_type='yolov8',\n",
        "    model_path=ckpt_path,\n",
        "    confidence_threshold=0.25, ## same as the default value for YOLOv8 model\n",
        "    image_size=640,\n",
        "    device=\"cpu\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NvJ6zB9omBJS"
      },
      "outputs": [],
      "source": [
        "# Define function for prediction\n",
        "def predict_with_slicing(sample, label_field, **kwargs):\n",
        "    result = get_sliced_prediction(\n",
        "        sample.filepath, detection_model, verbose=0, **kwargs\n",
        "    )\n",
        "    sample[label_field] = fo.Detections(detections=result.to_fiftyone_detections())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JUe3tvxqmBJS"
      },
      "outputs": [],
      "source": [
        "kwargs = {\"overlap_height_ratio\": 0.2, \"overlap_width_ratio\": 0.2}\n",
        "\n",
        "for sample in dataset.iter_samples(progress=True, autosave=True):\n",
        "    predict_with_slicing(sample, label_field=\"small_slices\", slice_height=320, slice_width=320, **kwargs)\n",
        "    predict_with_slicing(sample, label_field=\"large_slices\", slice_height=480, slice_width=480, **kwargs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K822hcrAmBJS"
      },
      "source": [
        "These inference times are much longer than the original inference time. This is because the model running on multiple slices *per* image, which increases the number of forward passes the model has to make. This is a trade-off when making to improve the detection of small objects."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MtRl4fFrmBJS"
      },
      "outputs": [],
      "source": [
        "filtered_view = filter_all_labels(mapped_view).take(50, 50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DmEcQrr-mBJS"
      },
      "outputs": [],
      "source": [
        "session = fo.launch_app(filtered_view, auto=False)\n",
        "session.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v7i_Cb6vmBJT"
      },
      "source": [
        "![Sliced Model Predictions](https://github.com/voxel51/fiftyone/blob/v0.24.1/docs/source/tutorials/images/sahi_slices.gif?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yzZst4JomBJT"
      },
      "source": [
        "The results certainly look promising! From a few visual examples, slicing seems to improve the coverage of ground truth detections, and smaller slices in particular seem to lead to more of the `person` detections being captured."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n2L9MtbDmBJT"
      },
      "source": [
        "## Evaluating SAHI Predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03r5s3iVmBJT"
      },
      "source": [
        "Running evaluation routine comparing our predictions from each of the prediction label fields to the ground truth labels. The `evaluate_detections()` method will mark each detection as a true positive, false positive, or false negative. Here the default IoU threshold is $0.5$, but it can be adjusted as needed:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Of3xhzUtmBJT"
      },
      "outputs": [],
      "source": [
        "base_results = filtered_view.evaluate_detections(\"base_model\", gt_field=\"ground_truth\", eval_key=\"eval_base_model\")\n",
        "large_slice_results = filtered_view.evaluate_detections(\"large_slices\", gt_field=\"ground_truth\", eval_key=\"eval_large_slices\")\n",
        "small_slice_results = filtered_view.evaluate_detections(\"small_slices\", gt_field=\"ground_truth\", eval_key=\"eval_small_slices\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xS3T3Ey_mBJT"
      },
      "outputs": [],
      "source": [
        "print(\"Base model results:\")\n",
        "base_results.print_report()\n",
        "\n",
        "print(\"-\" * 50)\n",
        "print(\"Large slice results:\")\n",
        "large_slice_results.print_report()\n",
        "\n",
        "print(\"-\" * 50)\n",
        "print(\"Small slice results:\")\n",
        "small_slice_results.print_report()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kJPM4j4vmBJT"
      },
      "source": [
        "We can see that as we introduce more slices, the number of false positives increases, while the number of false negatives decreases. This is expected, as the model is able to detect more objects with more slices, but also makes more mistakes! To minimize false positives, more agressive confidence thresholding can be applied, but even without doing this the $F_1$-score has significantly improved."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Video inference"
      ],
      "metadata": {
        "id": "X5CV5sJZYVk9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Standard Inference with YOLOv8"
      ],
      "metadata": {
        "id": "7Vmig8c6pWgk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "eTT2FZZXuv6d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the path to your video file on Google Drive\n",
        "video_path = '/content/drive/MyDrive/Deakin_open_day/Open Day at Deakin University (online-video-cutter.com).mp4'\n",
        "\n",
        "# Load the YOLOv8 model\n",
        "model_video = YOLO('yolov8n.pt')\n",
        "\n",
        "# Perform object detection on the video\n",
        "model_video.predict(\n",
        "    source=video_path,\n",
        "    save=True,\n",
        "    project='runs/detect',\n",
        "    name='predict1',\n",
        "    exist_ok=True\n",
        ")\n",
        "\n",
        "# Display the annotated video\n",
        "output_dir = 'runs/detect/predict1'\n",
        "output_files = os.listdir(output_dir)\n",
        "print('Files in output directory:', output_files)\n",
        "\n",
        "# Find the output video file\n",
        "annotated_video = None\n",
        "for file in output_files:\n",
        "    if file.endswith('.mp4') or file.endswith('.avi'):\n",
        "        annotated_video = os.path.join(output_dir, file)\n",
        "        break\n",
        "\n",
        "if annotated_video:\n",
        "    display(Video(annotated_video, embed=True))\n",
        "else:\n",
        "    print('No output video found.')\n",
        "\n",
        "# Optional: Save the annotated video back to Google Drive\n",
        "import shutil\n",
        "\n",
        "drive_output_path = '/content/drive/MyDrive/Deakin_open_day/yolov8_annotated_video.mp4'\n",
        "shutil.copy(annotated_video, drive_output_path)"
      ],
      "metadata": {
        "id": "opXzdTnWvFVk",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inference with YOLOv8 + SAHI"
      ],
      "metadata": {
        "id": "dtBjCvSNKUu2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since the SAHI framework is designed to detect objects in images, it haven't yet to directly process videos for inference like the YOLO model. Therefore, we need to extract frames from the video, run the object detection on each frame, and then reassemble the frames back into a video. This process can be somewhat complex. However, Ultralytics provides a pre-built package for object detection on videos, which simplifies the task. A more efficient approach is to clone their GitHub repository and execute the detection using the provided command-line tools, which also saves considerable computation time  (Ultralytics, 2023)."
      ],
      "metadata": {
        "id": "fy_1MWbkKvaz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Clone the Ultralytics repository\n",
        "!git clone https://github.com/ultralytics/ultralytics.git"
      ],
      "metadata": {
        "id": "FjSOj2Td0fBF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/ultralytics/examples/YOLOv8-SAHI-Inference-Video"
      ],
      "metadata": {
        "id": "Hamibeso1cJz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the command string\n",
        "cmd = f'python yolov8_sahi.py --source \"{video_path}\" --save-img'\n",
        "\n",
        "# Execute the command\n",
        "get_ipython().system(cmd)"
      ],
      "metadata": {
        "id": "-XiXW_gYoS6N",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_dir = 'ultralytics_results_with_sahi/exp'  # This is the default directory in the script\n",
        "print('Files in output directory:', os.listdir(output_dir))"
      ],
      "metadata": {
        "id": "GN87LU6x3X4U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "output_files = os.listdir(output_dir)\n",
        "annotated_video = None\n",
        "for file in output_files:\n",
        "    if file.endswith('.mp4') or file.endswith('.avi'):\n",
        "        annotated_video = os.path.join(output_dir, file)\n",
        "        break\n",
        "\n",
        "if annotated_video:\n",
        "    print(f\"Annotated video path: {annotated_video}\")\n",
        "    display(Video(annotated_video, embed=True))\n",
        "else:\n",
        "    print('No output video found.')\n"
      ],
      "metadata": {
        "id": "SLam0A8K3kDQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def create_side_by_side_video(video1_path, video2_path, output_path):\n",
        "    # Open the video files\n",
        "    cap1 = cv2.VideoCapture(video1_path)\n",
        "    cap2 = cv2.VideoCapture(video2_path)\n",
        "\n",
        "    # Check if videos opened successfully\n",
        "    if not cap1.isOpened():\n",
        "        print(f\"Error opening video file {video1_path}\")\n",
        "        return\n",
        "    if not cap2.isOpened():\n",
        "        print(f\"Error opening video file {video2_path}\")\n",
        "        return\n",
        "\n",
        "    # Get properties from the first video\n",
        "    fps1 = cap1.get(cv2.CAP_PROP_FPS)\n",
        "    width1 = int(cap1.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    height1 = int(cap1.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "\n",
        "    # Get properties from the second video\n",
        "    fps2 = cap2.get(cv2.CAP_PROP_FPS)\n",
        "    width2 = int(cap2.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    height2 = int(cap2.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "\n",
        "    # Use the minimum FPS of the two videos to avoid speeding up any video\n",
        "    fps = min(fps1, fps2)\n",
        "\n",
        "    # Define the codec and create VideoWriter object\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "\n",
        "    # Output video size will be (width1 + width2) x max(height1, height2)\n",
        "    output_width = width1 + width2\n",
        "    output_height = max(height1, height2)\n",
        "\n",
        "    out = cv2.VideoWriter(output_path, fourcc, fps, (output_width, output_height))\n",
        "\n",
        "    while True:\n",
        "        # Read frames from both videos\n",
        "        ret1, frame1 = cap1.read()\n",
        "        ret2, frame2 = cap2.read()\n",
        "\n",
        "        # Break the loop if any video ends\n",
        "        if not ret1 or not ret2:\n",
        "            break\n",
        "\n",
        "        # Resize frames to have the same height\n",
        "        if height1 != height2:\n",
        "            # Calculate the scaling factors\n",
        "            scale_factor1 = output_height / height1\n",
        "            scale_factor2 = output_height / height2\n",
        "\n",
        "            # Resize frames\n",
        "            frame1 = cv2.resize(frame1, (int(width1 * scale_factor1), output_height))\n",
        "            frame2 = cv2.resize(frame2, (int(width2 * scale_factor2), output_height))\n",
        "\n",
        "        # Concatenate frames horizontally\n",
        "        combined_frame = cv2.hconcat([frame1, frame2])\n",
        "\n",
        "        # Write the combined frame to the output video\n",
        "        out.write(combined_frame)\n",
        "\n",
        "    # Release all resources\n",
        "    cap1.release()\n",
        "    cap2.release()\n",
        "    out.release()\n",
        "    print(\"left is YOLOv8, right is YOLO + SAHI prediction\")\n",
        "\n",
        "# Example usage\n",
        "video1_path = '/content/drive/MyDrive/Deakin_open_day/yolov8_annotated_video.mp4'\n",
        "video2_path = '/content/ultralytics/examples/YOLOv8-SAHI-Inference-Video/ultralytics_results_with_sahi/exp/Open Day at Deakin University (online-video-cutter.com).mp4'\n",
        "output_path = 'side_by_side_output.mp4'\n",
        "\n",
        "create_side_by_side_video(video1_path, video2_path, output_path)\n",
        "\n",
        "# Display the video in the notebook\n",
        "Video(output_path, embed=True, width=800)\n"
      ],
      "metadata": {
        "id": "MSKel-B0-_Zk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Show the results\n",
        "# Replace with your YouTube video ID\n",
        "youtube_id = 'xTj8JKMn0_4'\n",
        "\n",
        "# Display the YouTube video\n",
        "YouTubeVideo(youtube_id, width=560, height=315)"
      ],
      "metadata": {
        "id": "02NneMR6Jslf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fbu5s_vSmBJU"
      },
      "source": [
        "# Limitation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PeIclLX5mBJU"
      },
      "source": [
        "Although SAHI is a great framework to comprehense detect objects, however the trade off is long computing time (it's approx. 10 times longer than predict with the pretrain YOLOv8)\n",
        "To maximize the effectiveness of SAHI, there're few experiment they suggest to do:\n",
        "\n",
        "- Slicing hyperparameters, such as slice height and width, and overlap.\n",
        "- Base object detection models, as SAHI is compatible with many models, including YOLOv5, and Hugging Face Transformers models.\n",
        "- Confidence thresholding (potentially on a class-by-class basis), to reduce the number of false positives.\n",
        "- Post-processing techniques, such as [non-maximum suppression (NMS)](https://docs.voxel51.com/api/fiftyone.utils.labels.html#fiftyone.utils.labels.perform_nms), to reduce the number of overlapping detections.\n",
        "- Human-in-the-loop (HITL) workflows, to correct ground truth labels."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZjUEOVQ5mBJU"
      },
      "source": [
        "# References\n",
        "\n",
        "1. Marks, J. (2024). *How to Detect Small Objects*. Voxel51. [Link to Article](https://voxel51.com/blog/how-to-detect-small-objects/)\n",
        "2. Jocher, G. & Rizwan, M. (2023).  *Ultralytics Docs: Using YOLOv8 with SAHI for Sliced Inference*. Ultralytics YOLO Docs. [Link to Article](https://docs.ultralytics.com/guides/sahi-tiled-inference/)\n",
        "\n",
        "3. Ultralytics. (2023). *Using Ultralytics YOLOv8 with SAHI on videos*. [Link to Article](https://ultralytics.medium.com/using-ultralytics-yolov8-with-sahi-on-videos-3d524087dd33)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "toc_visible": true,
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}