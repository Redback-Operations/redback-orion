{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e661769-b47f-495a-a094-aa8bba344593",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2 as cv\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np  \n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58396553-5011-462f-a323-9ab7b3c121a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = cv.dnn.readNetFromTensorflow(\"graph_opt.pb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "56476780-acd7-4e5e-b277-e5258b404d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "inWidth = 416  \n",
    "inHeight = 416  \n",
    "thr = 0.1  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d8a63c02-3cb8-48bb-8f38-ea247a009abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the BODY_PARTS with names as keys\n",
    "BODY_PARTS = {\"Nose\": 0, \"Neck\": 1, \"RShoulder\": 2, \"RElbow\": 3, \"RWrist\": 4,\n",
    "              \"LShoulder\": 5, \"LElbow\": 6, \"LWrist\": 7, \"RHip\": 8, \"RKnee\": 9,\n",
    "              \"RAnkle\": 10, \"LHip\": 11, \"LKnee\": 12, \"LAnkle\": 13, \"REye\": 14,\n",
    "              \"LEye\": 15, \"REar\": 16, \"LEar\": 17, \"Background\": 18}\n",
    "\n",
    "POSE_PAIRS = [[\"Neck\", \"RShoulder\"], [\"Neck\", \"LShoulder\"], [\"RShoulder\", \"RElbow\"],\n",
    "              [\"RElbow\", \"RWrist\"], [\"LShoulder\", \"LElbow\"], [\"LElbow\", \"LWrist\"],\n",
    "              [\"Neck\", \"RHip\"], [\"RHip\", \"RKnee\"], [\"RKnee\", \"RAnkle\"],\n",
    "              [\"Neck\", \"LHip\"], [\"LHip\", \"LKnee\"], [\"LKnee\", \"LAnkle\"],\n",
    "              [\"Neck\", \"Nose\"], [\"Nose\", \"REye\"], [\"REye\", \"REar\"],\n",
    "              [\"Nose\", \"LEye\"], [\"LEye\", \"LEar\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "762f5b37-780a-4fc2-b5c6-89508932b0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv.imread(\"data/pose1.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "539271e3-69dd-4469-9f51-c699850dfd83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8e4382b1-4349-41ee-80b6-b8a517c9a5b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "plt.imshow(cv.cvtColor(img, cv.COLOR_BGR2RGB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2e29a0ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    \"frame\": [],\n",
    "    \"processing_time_ms\": [],\n",
    "    \"detected_keypoints\": [],\n",
    "    \"confidence_scores\": []\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4e783d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_filenames = [\n",
    "    \"data/pose.jpg\", \"data/pose1.jpg\", \"data/pose2.jpg\", \"data/pose3.jpg\", \"data/pose4.jpg\", \"data/posee.jpg\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "797dc236",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the yolo_pose_estimation function\n",
    "def yolo_pose_estimation(image):\n",
    "    model = torch.hub.load('ultralytics/yolov5', 'yolov5s')\n",
    "    results = model(image)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0a070c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The `classify_pose` function is designed to classify the pose of a person based on the detected keypoints \n",
    "# from a pose estimation model. It takes a list of keypoints as input and returns a description of the pose. \n",
    "# The function checks the positions of various body parts, such as shoulders, elbows, hips, knees, and ankles, \n",
    "# to determine specific poses like \"Arms Outward\", \"Legs Straight\", \"T-Pose\", \"Squat\", \"Running\", \"Standing\", and \"Jumping\". \n",
    "# This function can be extended by adding more conditions to classify additional poses or by refining the existing \n",
    "# conditions for better accuracy. To use this function, pass the detected keypoints from the pose estimation model to it, \n",
    "# and it will return a string describing the pose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1cc4aeec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the classify_pose function\n",
    "def classify_pose(points):\n",
    "    \"\"\"Classify the pose based on keypoints for sports and fitness applications.\"\"\"\n",
    "    pose_description = []\n",
    "\n",
    "    # Check arms position\n",
    "    if points[BODY_PARTS[\"RShoulder\"]] and points[BODY_PARTS[\"LShoulder\"]] and \\\n",
    "       points[BODY_PARTS[\"RElbow\"]] and points[BODY_PARTS[\"LElbow\"]]:\n",
    "        shoulder_distance = np.linalg.norm(np.array(points[BODY_PARTS[\"RShoulder\"]]) - np.array(points[BODY_PARTS[\"LShoulder\"]]))\n",
    "        elbow_distance = np.linalg.norm(np.array(points[BODY_PARTS[\"RElbow\"]]) - np.array(points[BODY_PARTS[\"LElbow\"]]))\n",
    "\n",
    "        if elbow_distance > shoulder_distance * 1.5:  # Arms stretched outward\n",
    "            pose_description.append(\"Arms Outward\")\n",
    "        elif elbow_distance < shoulder_distance * 0.75:  # Arms close to body\n",
    "            pose_description.append(\"Arms Neutral\")\n",
    "        else:\n",
    "            pose_description.append(\"Arms Unknown\")\n",
    "    else:\n",
    "        pose_description.append(\"Arms Unknown\")\n",
    "\n",
    "    # Check legs position\n",
    "    if points[BODY_PARTS[\"RHip\"]] and points[BODY_PARTS[\"LHip\"]] and \\\n",
    "       points[BODY_PARTS[\"RKnee\"]] and points[BODY_PARTS[\"LKnee\"]]:\n",
    "        hip_distance = np.linalg.norm(np.array(points[BODY_PARTS[\"RHip\"]]) - np.array(points[BODY_PARTS[\"LHip\"]]))\n",
    "        right_leg_straight = points[BODY_PARTS[\"RKnee\"]] and points[BODY_PARTS[\"RKnee\"]][1] > points[BODY_PARTS[\"RHip\"]][1] and \\\n",
    "                             points[BODY_PARTS[\"RAnkle\"]] and points[BODY_PARTS[\"RAnkle\"]][1] > points[BODY_PARTS[\"RKnee\"]][1]\n",
    "        left_leg_straight = points[BODY_PARTS[\"LKnee\"]] and points[BODY_PARTS[\"LKnee\"]][1] > points[BODY_PARTS[\"LHip\"]][1] and \\\n",
    "                            points[BODY_PARTS[\"LAnkle\"]] and points[BODY_PARTS[\"LAnkle\"]][1] > points[BODY_PARTS[\"LKnee\"]][1]\n",
    "\n",
    "        if right_leg_straight and left_leg_straight:  # Both legs straight\n",
    "            pose_description.append(\"Legs Straight\")\n",
    "        elif not right_leg_straight or not left_leg_straight:  # One or both legs bent\n",
    "            pose_description.append(\"Legs Bent\")\n",
    "        else:\n",
    "            pose_description.append(\"Legs Unknown\")\n",
    "    else:\n",
    "        pose_description.append(\"Legs Unknown\")\n",
    "\n",
    "    # Check for specific sports and fitness poses\n",
    "    if \"Arms Outward\" in pose_description and \"Legs Straight\" in pose_description:\n",
    "        pose_description.append(\"T-Pose\")\n",
    "    if \"Arms Neutral\" in pose_description and \"Legs Bent\" in pose_description:\n",
    "        pose_description.append(\"Squat\")\n",
    "    if points[BODY_PARTS[\"RShoulder\"]] and points[BODY_PARTS[\"LShoulder\"]] and \\\n",
    "       points[BODY_PARTS[\"RHip\"]] and points[BODY_PARTS[\"LHip\"]] and \\\n",
    "       points[BODY_PARTS[\"RKnee\"]] and points[BODY_PARTS[\"LKnee\"]]:\n",
    "        if points[BODY_PARTS[\"RShoulder\"]][1] < points[BODY_PARTS[\"RHip\"]][1] and \\\n",
    "           points[BODY_PARTS[\"LShoulder\"]][1] < points[BODY_PARTS[\"LHip\"]][1] and \\\n",
    "           points[BODY_PARTS[\"RKnee\"]][1] < points[BODY_PARTS[\"RHip\"]][1] and \\\n",
    "           points[BODY_PARTS[\"LKnee\"]][1] < points[BODY_PARTS[\"LHip\"]][1]:\n",
    "            pose_description.append(\"Jumping Jack\")\n",
    "\n",
    "    # Additional checks for sports and fitness applications\n",
    "    if points[BODY_PARTS[\"RShoulder\"]] and points[BODY_PARTS[\"LShoulder\"]] and \\\n",
    "       points[BODY_PARTS[\"RHip\"]] and points[BODY_PARTS[\"LHip\"]]:\n",
    "        if points[BODY_PARTS[\"RShoulder\"]][1] < points[BODY_PARTS[\"RHip\"]][1] and \\\n",
    "           points[BODY_PARTS[\"LShoulder\"]][1] < points[BODY_PARTS[\"LHip\"]][1]:\n",
    "            if points[BODY_PARTS[\"RKnee\"]] and points[BODY_PARTS[\"LKnee\"]]:\n",
    "                if points[BODY_PARTS[\"RKnee\"]][1] < points[BODY_PARTS[\"RHip\"]][1] and \\\n",
    "                   points[BODY_PARTS[\"LKnee\"]][1] < points[BODY_PARTS[\"LHip\"]][1]:\n",
    "                    pose_description.append(\"Running\")\n",
    "                if points[BODY_PARTS[\"RKnee\"]][1] > points[BODY_PARTS[\"RHip\"]][1] and \\\n",
    "                   points[BODY_PARTS[\"LKnee\"]][1] > points[BODY_PARTS[\"LHip\"]][1]:\n",
    "                    pose_description.append(\"Standing\")\n",
    "\n",
    "    if points[BODY_PARTS[\"RShoulder\"]] and points[BODY_PARTS[\"LShoulder\"]] and \\\n",
    "       points[BODY_PARTS[\"RHip\"]] and points[BODY_PARTS[\"LHip\"]] and \\\n",
    "       points[BODY_PARTS[\"RKnee\"]] and points[BODY_PARTS[\"LKnee\"]] and \\\n",
    "       points[BODY_PARTS[\"RAnkle\"]] and points[BODY_PARTS[\"LAnkle\"]]:\n",
    "        if points[BODY_PARTS[\"RAnkle\"]][1] < points[BODY_PARTS[\"RKnee\"]][1] and \\\n",
    "           points[BODY_PARTS[\"LAnkle\"]][1] < points[BODY_PARTS[\"LKnee\"]][1]:\n",
    "            pose_description.append(\"Jumping\")\n",
    "\n",
    "    # Check for general posture\n",
    "    if points[BODY_PARTS[\"Neck\"]] and points[BODY_PARTS[\"RHip\"]] and points[BODY_PARTS[\"LHip\"]]:\n",
    "        neck = points[BODY_PARTS[\"Neck\"]]\n",
    "        rhip = points[BODY_PARTS[\"RHip\"]]\n",
    "        lhip = points[BODY_PARTS[\"LHip\"]]\n",
    "\n",
    "        if abs(neck[0] - rhip[0]) < 20 and abs(neck[0] - lhip[0]) < 20:\n",
    "            pose_description.append(\"Standing Straight\")\n",
    "        elif neck[0] < rhip[0] and neck[0] < lhip[0]:\n",
    "            pose_description.append(\"Leaning Left\")\n",
    "        elif neck[0] > rhip[0] and neck[0] > lhip[0]:\n",
    "            pose_description.append(\"Leaning Right\")\n",
    "\n",
    "    # Check for sitting posture\n",
    "    if points[BODY_PARTS[\"Neck\"]] and points[BODY_PARTS[\"RHip\"]] and points[BODY_PARTS[\"LHip\"]]:\n",
    "        if points[BODY_PARTS[\"Neck\"]][1] > points[BODY_PARTS[\"RHip\"]][1] and \\\n",
    "           points[BODY_PARTS[\"Neck\"]][1] > points[BODY_PARTS[\"LHip\"]][1]:\n",
    "            pose_description.append(\"Sitting\")\n",
    "\n",
    "    # Combine pose descriptions\n",
    "    return \"Pose: \" + \", \".join(pose_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "78f84b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The `pose_estimation` function uses the OpenPose model to estimate the pose of a person in a given frame. \n",
    "# It takes an image frame and a frame ID as input and returns the processed frame with the pose skeleton drawn on it and the pose label. \n",
    "# The function first checks if the required model files exist, then loads the OpenPose model. \n",
    "# It prepares the frame for pose estimation, performs the forward pass to get the keypoints, and classifies the pose based on the detected keypoints. \n",
    "# The skeleton lines and points are drawn on the frame, and performance profiling data is collected. \n",
    "# This function can be extended by adding more pose classifications or refining the existing ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "adbc2c3f-69ae-45fd-8d99-e70648c6bff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the pose_estimation function using OpenPose\n",
    "def pose_estimation(frame, frame_id):\n",
    "    protoFile = \"pose_deploy_linevec.prototxt\"\n",
    "    weightsFile = \"pose_iter_440000.caffemodel\"\n",
    "\n",
    "    # Check if the model files exist\n",
    "    if not os.path.isfile(protoFile) or not os.path.isfile(weightsFile):\n",
    "        raise FileNotFoundError(\"Model files not found. Please ensure 'pose_deploy_linevec.prototxt' and 'pose_iter_440000.caffemodel' are in the working directory.\")\n",
    "\n",
    "    net = cv.dnn.readNetFromCaffe(protoFile, weightsFile)\n",
    "\n",
    "    frameWidth = frame.shape[1]\n",
    "    frameHeight = frame.shape[0]\n",
    "\n",
    "    # Start timing\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Prepare the frame for pose estimation\n",
    "    inpBlob = cv.dnn.blobFromImage(frame, 1.0 / 255, (368, 368), (0, 0, 0), swapRB=False, crop=False)\n",
    "    net.setInput(inpBlob)\n",
    "    output = net.forward()\n",
    "\n",
    "    H = output.shape[2]\n",
    "    W = output.shape[3]\n",
    "\n",
    "    # Extract keypoints and confidence scores\n",
    "    points = []\n",
    "    detected_keypoints = 0\n",
    "    confidence_scores = []\n",
    "\n",
    "    for i in range(len(BODY_PARTS)):\n",
    "        heatMap = output[0, i, :, :]\n",
    "        _, conf, _, point = cv.minMaxLoc(heatMap)\n",
    "        x = (frameWidth * point[0]) / W\n",
    "        y = (frameHeight * point[1]) / H\n",
    "\n",
    "        # Add a point if its confidence is higher than threshold.\n",
    "        if conf > 0.1:  # Assuming a threshold value\n",
    "            detected_keypoints += 1\n",
    "            confidence_scores.append(conf)\n",
    "            points.append((int(x), int(y)))\n",
    "        else:\n",
    "            points.append(None)\n",
    "\n",
    "    # Classify the pose\n",
    "    pose_label = classify_pose(points)\n",
    "\n",
    "    # Draw the skeleton lines and points\n",
    "    for pair in POSE_PAIRS:\n",
    "        partFrom = pair[0]\n",
    "        partTo = pair[1]\n",
    "        assert(partFrom in BODY_PARTS)\n",
    "        assert(partTo in BODY_PARTS)\n",
    "\n",
    "        idFrom = BODY_PARTS[partFrom]\n",
    "        idTo = BODY_PARTS[partTo]\n",
    "\n",
    "        if points[idFrom] and points[idTo]:\n",
    "            cv.line(frame, points[idFrom], points[idTo], (0, 255, 0), 3)\n",
    "            cv.ellipse(frame, points[idFrom], (3, 3), 0, 0, 360, (0, 0, 255), cv.FILLED)\n",
    "            cv.ellipse(frame, points[idTo], (3, 3), 0, 0, 360, (0, 0, 255), cv.FILLED)\n",
    "\n",
    "    # Performance profiling\n",
    "    processing_time = (time.time() - start_time) * 1000  # Processing time in ms\n",
    "\n",
    "    # Save data for this frame\n",
    "    data[\"frame\"].append(frame_id)\n",
    "    data[\"processing_time_ms\"].append(processing_time)\n",
    "    data[\"detected_keypoints\"].append(detected_keypoints)\n",
    "    data[\"confidence_scores\"].append(confidence_scores)\n",
    "\n",
    "    return frame, pose_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1ae045da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The following code processes a list of images to perform pose estimation using the YOLO and OpenPose models. \n",
    "# It loads each image, performs pose estimation, and saves the results with a title indicating the detected pose. \n",
    "# The processed images are saved in the specified output directory, and the data collected during processing is saved to a CSV file. \n",
    "# This code can be extended by adding more images to the list or modifying the pose estimation function to include additional pose classifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0a7dbac8-b222-473f-8e2b-8677aaf693dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Process all images\n",
    "output_dir = \"output\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "for idx, image_filename in enumerate(image_filenames, 1):\n",
    "    # Load the image\n",
    "    img = cv.imread(image_filename)\n",
    "    \n",
    "    if img is None:\n",
    "        print(f\"Error: Unable to load image {image_filename}\")\n",
    "        continue\n",
    "\n",
    "    # Process the frame\n",
    "    estimated_image, pose_label = pose_estimation(img, idx)\n",
    "\n",
    "    # Save the result with title\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.imshow(cv.cvtColor(estimated_image, cv.COLOR_BGR2RGB))\n",
    "    plt.title(pose_label)\n",
    "    plt.axis('off')\n",
    "    output_filename = os.path.join(output_dir, f\"pose_estimation_{idx}.png\")\n",
    "    plt.savefig(output_filename, bbox_inches='tight', pad_inches=0.1)\n",
    "    plt.close()\n",
    "    print(f\"Saved: {output_filename}\")\n",
    "\n",
    "# After processing all frames, save the data to CSV\n",
    "data_df = pd.DataFrame(data)\n",
    "data_df.to_csv(\"pose_data.csv\", index=False)\n",
    "print(\"Data saved to pose_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8ae4975b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "import os\n",
    "\n",
    "def display_saved_images(output_dir=\"output\"):\n",
    "    for filename in sorted(os.listdir(output_dir)):\n",
    "        if filename.endswith(\".png\"):\n",
    "            display(Image(filename=os.path.join(output_dir, filename)))\n",
    "\n",
    "# call image display funciton\n",
    "display_saved_images()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "edc5a5a8-6a36-4d48-8dab-d17ebf34243f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "plt.imshow(cv.cvtColor(img, cv.COLOR_BGR2RGB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a09498e-7726-46ae-a5c3-2c372c9d6f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Video\n",
    "\n",
    "#This code performs real-time pose estimation on a video using OpenCV’s DNN (Deep Neural Network) module with a pre-trained TensorFlow model. \n",
    "#It starts by attempting to open a video file (pose_estimation_video.mp4), and if unsuccessful, it defaults to capturing video from the system's webcam. \n",
    "#The video feed is processed frame by frame in a loop, where each frame is converted into a blob format suitable for the neural network using cv.dnn.blobFromImage(). \n",
    "#The pre-trained model performs a forward pass on the frame, generating keypoints for different body parts. \n",
    "#These keypoints are extracted based on confidence values, and if they meet the threshold, they are added to a list. \n",
    "#Lines are drawn between connected keypoints to form a skeleton, and ellipses are placed at the keypoint locations to visualize the detected body parts. \n",
    "#Additionally, the code measures the time taken to process each frame and displays it in milliseconds on the video feed. \n",
    "#Finally, the processed video frames with the overlaid skeleton are displayed in real-time using cv.imshow().\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a54baf-77fb-4560-b1e6-724579f86b82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Perform this demo on a video\n",
    "cap = cv.VideoCapture('data/pose_estimation_video.mp4')\n",
    "cap.set(3, 800)  # Set height\n",
    "cap.set(4, 800)  # Set width\n",
    "\n",
    "if not cap.isOpened():\n",
    "    cap = cv.VideoCapture(0)\n",
    "if not cap.isOpened():\n",
    "    raise IOError(\"Cannot open video\")\n",
    "\n",
    "# Initialize data collection for analytics\n",
    "data = {\n",
    "    \"frame\": [],\n",
    "    \"processing_time_ms\": [],\n",
    "    \"detected_keypoints\": [],\n",
    "    \"confidence_scores\": []\n",
    "}\n",
    "\n",
    "frame_id = 0\n",
    "\n",
    "while cv.waitKey(1) < 0:\n",
    "    hasFrame, frame = cap.read()\n",
    "    if not hasFrame:\n",
    "        cv.waitKey()\n",
    "        break\n",
    "\n",
    "    frame_id += 1\n",
    "    frameWidth = frame.shape[1]\n",
    "    frameHeight = frame.shape[0]\n",
    "\n",
    "    # Start timing\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Perform pose estimation\n",
    "    net.setInput(cv.dnn.blobFromImage(frame, 1.0, (inWidth, inHeight), (127.5, 127.5, 127.5), swapRB=True, crop=False))\n",
    "    out = net.forward()\n",
    "    out = out[:, :19, :, :]\n",
    "\n",
    "    assert(len(BODY_PARTS) == out.shape[1])\n",
    "\n",
    "    points = []\n",
    "    detected_keypoints = 0\n",
    "    confidence_scores = []\n",
    "\n",
    "    for i in range(len(BODY_PARTS)):\n",
    "        heatMap = out[0, i, :, :]\n",
    "        _, conf, _, point = cv.minMaxLoc(heatMap)\n",
    "        x = (frameWidth * point[0]) / out.shape[3]\n",
    "        y = (frameHeight * point[1]) / out.shape[2]\n",
    "\n",
    "        # Add a point if its confidence is higher than threshold\n",
    "        if conf > thr:\n",
    "            detected_keypoints += 1\n",
    "            confidence_scores.append(conf)\n",
    "            points.append((int(x), int(y)))\n",
    "        else:\n",
    "            points.append(None)\n",
    "\n",
    "    # Draw the skeleton lines and points\n",
    "    for pair in POSE_PAIRS:\n",
    "        partFrom = pair[0]\n",
    "        partTo = pair[1]\n",
    "        assert(partFrom in BODY_PARTS)\n",
    "        assert(partTo in BODY_PARTS)\n",
    "\n",
    "        idFrom = BODY_PARTS[partFrom]\n",
    "        idTo = BODY_PARTS[partTo]\n",
    "\n",
    "        if points[idFrom] and points[idTo]:\n",
    "            cv.line(frame, points[idFrom], points[idTo], (0, 255, 0), 3)\n",
    "            cv.ellipse(frame, points[idFrom], (3, 3), 0, 0, 360, (0, 0, 255), cv.FILLED)\n",
    "            cv.ellipse(frame, points[idTo], (3, 3), 0, 0, 360, (0, 0, 255), cv.FILLED)\n",
    "\n",
    "    # End timing\n",
    "    processing_time = (time.time() - start_time) * 1000  # Processing time in ms\n",
    "\n",
    "    # Save analytics data for this frame\n",
    "    data[\"frame\"].append(frame_id)\n",
    "    data[\"processing_time_ms\"].append(processing_time)\n",
    "    data[\"detected_keypoints\"].append(detected_keypoints)\n",
    "    data[\"confidence_scores\"].append(confidence_scores)\n",
    "\n",
    "    # Display processing time on the frame\n",
    "    cv.putText(frame, '%.2fms' % processing_time, (10, 20), cv.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0))\n",
    "\n",
    "    # Show the frame\n",
    "    cv.imshow('Pose Estimation Tutorial', frame)\n",
    "\n",
    "# Save the collected data to a CSV file\n",
    "data_df = pd.DataFrame(data)\n",
    "data_df.to_csv(\"video_pose_data.csv\", index=False)\n",
    "print(\"Data saved to video_pose_data.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed1efc7-ba2d-483e-a4f0-b5fd538cc9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This code performs real-time pose estimation using a webcam feed with OpenCV’s DNN module and a pre-trained TensorFlow model. \n",
    "#The script first attempts to open the camera feed from the secondary webcam (cv.VideoCapture(1)), and if this fails, it defaults to the primary camera (cv.VideoCapture(0)). \n",
    "#If no camera is available, an error is raised. The video feed is processed frame by frame in a loop. \n",
    "#For each frame, the image dimensions are obtained, and the frame is converted into a blob suitable for neural network input using cv.dnn.blobFromImage(). \n",
    "#The pose estimation model performs a forward pass on the frame, and keypoints for various body parts are extracted based on confidence scores. \n",
    "#These keypoints are stored and, if they meet a threshold, connected to form a skeleton using lines and ellipses. \n",
    "#The code also calculates and displays the inference time in milliseconds on the video frame. \n",
    "#The processed video with the overlaid skeleton is displayed in real-time using cv.imshow(). The loop runs until a key is pressed or the video feed is interrupted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a640d20-96a6-47da-93fa-524cb2cd1ad9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "#camera\n",
    "#perform this demo on video\n",
    "#from google.colab.patches import cv2_imshow\n",
    "cap = cv.VideoCapture(1)\n",
    "cap.set(3,800) #height\n",
    "cap.set(4,800) #width\n",
    "\n",
    "if not cap.isOpened():\n",
    "    cap = cv.VideoCapture(0)\n",
    "if not cap.isOpened():\n",
    "    raise IOError(\"Cannot open webcam\")\n",
    "\n",
    "while cv.waitKey(1) < 0:\n",
    "    hasFrame,frame = cap.read()\n",
    "    if not hasFrame:\n",
    "        cv.waitKey()\n",
    "        break\n",
    "        \n",
    "    frameWidth = frame.shape[1]\n",
    "    frameHeight = frame.shape[0]\n",
    "    net.setInput(cv.dnn.blobFromImage(frame, 1.0, (inWidth, inHeight), (127.5, 127.5, 127.5), swapRB=True, crop=False))\n",
    "    out = net.forward()\n",
    "    out = out[:, :19, :, :]\n",
    "\n",
    "    assert(len(BODY_PARTS) == out.shape[1])\n",
    "\n",
    "    points = []\n",
    "\n",
    "    for i in range(len(BODY_PARTS)):\n",
    "        heatMap = out[0, i, :, :]\n",
    "        _, conf, _, point = cv.minMaxLoc(heatMap)\n",
    "        print(f\"Body part {i}: Confidence = {conf}, Point = {point}\")\n",
    "        x = (frameWidth * point[0]) / out.shape[3]\n",
    "        y = (frameHeight * point[1]) / out.shape[2]\n",
    "\n",
    "        # Add a point if it's confidence is higher than threshold.\n",
    "        points.append((int(x), int(y)) if conf > thr else None)\n",
    "\n",
    "    for pair in POSE_PAIRS:\n",
    "        partFrom = pair[0]\n",
    "        partTo = pair[1]\n",
    "        assert(partFrom in BODY_PARTS)\n",
    "        assert(partTo in BODY_PARTS)\n",
    "\n",
    "        idFrom = BODY_PARTS[partFrom]\n",
    "        idTo = BODY_PARTS[partTo]\n",
    "\n",
    "        if points[idFrom] and points[idTo]:\n",
    "            cv.line(frame, points[idFrom], points[idTo], (0, 255, 0), 3)\n",
    "            cv.ellipse(frame, points[idFrom], (3, 3), 0, 0, 360, (0, 0, 255), cv.FILLED)\n",
    "            cv.ellipse(frame, points[idTo], (3, 3), 0, 0, 360, (0, 0, 255), cv.FILLED)\n",
    "\n",
    "    t, _ = net.getPerfProfile()\n",
    "    freq = cv.getTickFrequency() / 1000\n",
    "    cv.putText(frame, '%.2fms' % (t / freq), (10, 20), cv.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0))\n",
    "\n",
    "    cv.imshow('Pose Estimation Tutorial', frame)\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2463896-a7ff-40d7-9deb-eed09f4b6f70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "import cv2 as cv\n",
    "import numpy as np\n",
    "\n",
    "# Load the pre-trained neural network model\n",
    "net = cv.dnn.readNetFromTensorflow(\"graph_opt.pb\") \n",
    "\n",
    "# Set the input size for the network\n",
    "inWidth = 368\n",
    "inHeight = 368\n",
    "thr = 0.2  \n",
    "\n",
    "# Define the BODY_PARTS and POSE_PAIRS\n",
    "BODY_PARTS = {0: \"Nose\", 1: \"Neck\", 2: \"RShoulder\", 3: \"RElbow\", 4: \"RWrist\",\n",
    "              5: \"LShoulder\", 6: \"LElbow\", 7: \"LWrist\", 8: \"RHip\", 9: \"RKnee\",\n",
    "              10: \"RAnkle\", 11: \"LHip\", 12: \"LKnee\", 13: \"LAnkle\", 14: \"REye\",\n",
    "              15: \"LEye\", 16: \"REar\", 17: \"LEar\", 18: \"Background\"}\n",
    "\n",
    "POSE_PAIRS = [[\"Neck\", \"RShoulder\"], [\"Neck\", \"LShoulder\"], [\"RShoulder\", \"RElbow\"],\n",
    "              [\"RElbow\", \"RWrist\"], [\"LShoulder\", \"LElbow\"], [\"LElbow\", \"LWrist\"],\n",
    "              [\"Neck\", \"RHip\"], [\"RHip\", \"RKnee\"], [\"RKnee\", \"RAnkle\"],\n",
    "              [\"Neck\", \"LHip\"], [\"LHip\", \"LKnee\"], [\"LKnee\", \"LAnkle\"],\n",
    "              [\"Neck\", \"Nose\"], [\"Nose\", \"REye\"], [\"REye\", \"REar\"],\n",
    "              [\"Nose\", \"LEye\"], [\"LEye\", \"LEar\"]]\n",
    "\n",
    "# Define the pose_estimation function\n",
    "def pose_estimation(frame):\n",
    "    frameWidth = frame.shape[1]\n",
    "    frameHeight = frame.shape[0]\n",
    "    net.setInput(cv.dnn.blobFromImage(frame, 1.0, (inWidth, inHeight), (127.5, 127.5, 127.5), swapRB=True, crop=False))\n",
    "    out = net.forward()\n",
    "    out = out[:, :19, :, :]\n",
    "\n",
    "    assert(len(BODY_PARTS) == out.shape[1])\n",
    "\n",
    "    points = []\n",
    "\n",
    "    for i in range(len(BODY_PARTS)):\n",
    "        heatMap = out[0, i, :, :]\n",
    "        _, conf, _, point = cv.minMaxLoc(heatMap)\n",
    "        x = (frameWidth * point[0]) / out.shape[3]\n",
    "        y = (frameHeight * point[1]) / out.shape[2]\n",
    "\n",
    "        points.append((int(x), int(y)) if conf > thr else None)\n",
    "\n",
    "    return points\n",
    "\n",
    "# Function to calculate the Euclidean distance between two points\n",
    "def euclidean_distance(point1, point2):\n",
    "    return np.linalg.norm(np.array(point1) - np.array(point2))\n",
    "\n",
    "# Function to calculate the similarity percentage between two sets of keypoints\n",
    "def calculate_similarity_percentage(keypoints1, keypoints2, max_distance=200):\n",
    "    if len(keypoints1) != len(keypoints2):\n",
    "        raise ValueError(\"Keypoints from both images must have the same length.\")\n",
    "    \n",
    "    total_distance = 0\n",
    "    valid_points = 0\n",
    "\n",
    "    for kp1, kp2 in zip(keypoints1, keypoints2):\n",
    "        if kp1 is not None and kp2 is not None:\n",
    "            total_distance += euclidean_distance(kp1, kp2)\n",
    "            valid_points += 1\n",
    "\n",
    "    if valid_points == 0:\n",
    "        return 0  # No valid points to compare\n",
    "\n",
    "    average_distance = total_distance / valid_points\n",
    "    similarity_percentage = max(0, 100 * (1 - average_distance / max_distance))\n",
    "    \n",
    "    return similarity_percentage\n",
    "\n",
    "# Load the first image and extract keypoints\n",
    "img = cv.imread(\"pose2.jpg\")\n",
    "keypoints_img1 = pose_estimation(img)\n",
    "\n",
    "# Load the second image and extract keypoints\n",
    "img2 = cv.imread(\"pose.jpg\")\n",
    "keypoints_img2 = pose_estimation(img2)\n",
    "\n",
    "# Calculate the similarity percentage between the two sets of keypoints\n",
    "similarity_percentage = calculate_similarity_percentage(keypoints_img1, keypoints_img2)\n",
    "\n",
    "print(f\"The similarity between the two poses is {similarity_percentage:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b137a9-2dca-4701-86c1-c20927349ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This code performs pose estimation and matching on two images using OpenCV’s DNN module with a pre-trained TensorFlow model. \n",
    "#The pose_estimation function extracts keypoints for 18 body parts (like the nose, shoulders, and hips) from each image, and visualizes them by drawing skeletons using lines and ellipses. \n",
    "#The keypoints from both images are compared using Euclidean distance, and the calculate_similarity_percentage function calculates a similarity score between the two poses. \n",
    "#The images are resized to the same height for comparison, and the two skeleton-overlaid images are combined side by side. \n",
    "#The similarity percentage is displayed on the combined image, which is then shown using Matplotlib. \n",
    "#This allows for both a visual and quantitative assessment of how closely the two poses match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f05468b-ead6-4c79-84e0-7ae308672382",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "import cv2 as cv\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the pre-trained neural network model\n",
    "net = cv.dnn.readNetFromTensorflow(\"graph_opt.pb\")\n",
    "\n",
    "# Set the input size for the network\n",
    "inWidth = 368\n",
    "inHeight = 368\n",
    "thr = 0.2\n",
    "\n",
    "# Define the BODY_PARTS with names as keys\n",
    "BODY_PARTS = {\"Nose\": 0, \"Neck\": 1, \"RShoulder\": 2, \"RElbow\": 3, \"RWrist\": 4,\n",
    "              \"LShoulder\": 5, \"LElbow\": 6, \"LWrist\": 7, \"RHip\": 8, \"RKnee\": 9,\n",
    "              \"RAnkle\": 10, \"LHip\": 11, \"LKnee\": 12, \"LAnkle\": 13, \"REye\": 14,\n",
    "              \"LEye\": 15, \"REar\": 16, \"LEar\": 17, \"Background\": 18}\n",
    "\n",
    "POSE_PAIRS = [[\"Neck\", \"RShoulder\"], [\"Neck\", \"LShoulder\"], [\"RShoulder\", \"RElbow\"],\n",
    "              [\"RElbow\", \"RWrist\"], [\"LShoulder\", \"LElbow\"], [\"LElbow\", \"LWrist\"],\n",
    "              [\"Neck\", \"RHip\"], [\"RHip\", \"RKnee\"], [\"RKnee\", \"RAnkle\"],\n",
    "              [\"Neck\", \"LHip\"], [\"LHip\", \"LKnee\"], [\"LKnee\", \"LAnkle\"],\n",
    "              [\"Neck\", \"Nose\"], [\"Nose\", \"REye\"], [\"REye\", \"REar\"],\n",
    "              [\"Nose\", \"LEye\"], [\"LEye\", \"LEar\"]]\n",
    "\n",
    "# Pose estimation function with skeleton drawing\n",
    "def pose_estimation(frame):\n",
    "    if frame is None:\n",
    "        raise ValueError(\"The image could not be loaded. Please check the file path.\")\n",
    "    \n",
    "    frameWidth = frame.shape[1]\n",
    "    frameHeight = frame.shape[0]\n",
    "    net.setInput(cv.dnn.blobFromImage(frame, 1.0, (inWidth, inHeight), (127.5, 127.5, 127.5), swapRB=True, crop=False))\n",
    "    out = net.forward()\n",
    "    out = out[:, :19, :, :]\n",
    "\n",
    "    assert(len(BODY_PARTS) == out.shape[1])\n",
    "\n",
    "    points = []\n",
    "\n",
    "    for i in range(len(BODY_PARTS)):\n",
    "        heatMap = out[0, i, :, :]\n",
    "        _, conf, _, point = cv.minMaxLoc(heatMap)\n",
    "        x = (frameWidth * point[0]) / out.shape[3]\n",
    "        y = (frameHeight * point[1]) / out.shape[2]\n",
    "        \n",
    "        # Only consider points with confidence above threshold\n",
    "        points.append((int(x), int(y)) if conf > thr else None)\n",
    "\n",
    "    # Draw skeleton\n",
    "    for pair in POSE_PAIRS:\n",
    "        partFrom = pair[0]\n",
    "        partTo = pair[1]\n",
    "\n",
    "        idFrom = BODY_PARTS[partFrom]\n",
    "        idTo = BODY_PARTS[partTo]\n",
    "\n",
    "        if points[idFrom] and points[idTo]:\n",
    "            cv.line(frame, points[idFrom], points[idTo], (0, 255, 0), 3)\n",
    "            cv.ellipse(frame, points[idFrom], (3, 3), 0, 0, 360, (0, 0, 255), cv.FILLED)\n",
    "            cv.ellipse(frame, points[idTo], (3, 3), 0, 0, 360, (0, 0, 255), cv.FILLED)\n",
    "\n",
    "    return points, frame\n",
    "\n",
    "# Function to calculate Euclidean distance between two points\n",
    "def euclidean_distance(point1, point2):\n",
    "    return np.linalg.norm(np.array(point1) - np.array(point2))\n",
    "\n",
    "# Function to calculate similarity percentage between two sets of keypoints\n",
    "def calculate_similarity_percentage(keypoints1, keypoints2, max_distance=200):\n",
    "    total_distance = 0\n",
    "    valid_points = 0\n",
    "\n",
    "    for kp1, kp2 in zip(keypoints1, keypoints2):\n",
    "        if kp1 is not None and kp2 is not None:\n",
    "            total_distance += euclidean_distance(kp1, kp2)\n",
    "            valid_points += 1\n",
    "\n",
    "    if valid_points == 0:\n",
    "        return 0  # No valid points to compare\n",
    "\n",
    "    average_distance = total_distance / valid_points\n",
    "    similarity_percentage = max(0, 100 * (1 - average_distance / max_distance))\n",
    "    \n",
    "    return similarity_percentage\n",
    "\n",
    "# Function to resize images to the same height\n",
    "def resize_to_same_height(img1, img2):\n",
    "    height1 = img1.shape[0]\n",
    "    height2 = img2.shape[0]\n",
    "\n",
    "    if height1 != height2:\n",
    "        # Resize the second image to match the first image's height\n",
    "        img2 = cv.resize(img2, (int(img2.shape[1] * (height1 / height2)), height1))\n",
    "\n",
    "    return img1, img2\n",
    "\n",
    "# Load and process the first image\n",
    "img1 = cv.imread(\"pose4.jpg\")\n",
    "if img1 is None:\n",
    "    print(\"Error loading pose4.jpg\")\n",
    "\n",
    "keypoints_img1, img1_with_pose = pose_estimation(img1)\n",
    "\n",
    "# Load and process the second image\n",
    "img2 = cv.imread(\"pose4.jpg\")  # Changed file to \"pose2.jpg\" for comparison\n",
    "if img2 is None:\n",
    "    print(\"Error loading pose2.jpg\")\n",
    "\n",
    "keypoints_img2, img2_with_pose = pose_estimation(img2)\n",
    "\n",
    "# Resize images to have the same height\n",
    "img1_with_pose, img2_with_pose = resize_to_same_height(img1_with_pose, img2_with_pose)\n",
    "\n",
    "# Calculate the similarity percentage between the two poses\n",
    "similarity_percentage = calculate_similarity_percentage(keypoints_img1, keypoints_img2)\n",
    "\n",
    "# Combine both images side by side\n",
    "combined_image = np.hstack((img1_with_pose, img2_with_pose))\n",
    "\n",
    "# Add similarity percentage text to the combined image\n",
    "cv.putText(combined_image, f\"Similarity: {similarity_percentage:.2f}%\", (10, 30),\n",
    "           cv.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2)\n",
    "\n",
    "# Convert BGR to RGB for matplotlib\n",
    "combined_image_rgb = cv.cvtColor(combined_image, cv.COLOR_BGR2RGB)\n",
    "\n",
    "# Display the combined image using matplotlib\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.imshow(combined_image_rgb)\n",
    "plt.title(f\"Pose Estimation and Matching (Similarity: {similarity_percentage:.2f}%)\")\n",
    "plt.axis(\"off\")  # Hide the axis\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67204b3b-713d-4e2d-a9dd-80f87579ef91",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
