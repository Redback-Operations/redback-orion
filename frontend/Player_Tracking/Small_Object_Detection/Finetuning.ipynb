{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install sahi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CdNYCJ-FHhYz",
        "outputId": "7877a60d-2276-436e-d442-200fd3bf6fcd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sahi\n",
            "  Downloading sahi-0.11.19-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting opencv-python<=4.9.0.80 (from sahi)\n",
            "  Downloading opencv_python-4.9.0.80-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
            "Requirement already satisfied: shapely>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from sahi) (2.0.6)\n",
            "Requirement already satisfied: tqdm>=4.48.2 in /usr/local/lib/python3.10/dist-packages (from sahi) (4.66.6)\n",
            "Requirement already satisfied: pillow>=8.2.0 in /usr/local/lib/python3.10/dist-packages (from sahi) (11.0.0)\n",
            "Collecting pybboxes==0.1.6 (from sahi)\n",
            "  Downloading pybboxes-0.1.6-py3-none-any.whl.metadata (9.9 kB)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from sahi) (6.0.2)\n",
            "Collecting fire (from sahi)\n",
            "  Downloading fire-0.7.0.tar.gz (87 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.2/87.2 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting terminaltables (from sahi)\n",
            "  Downloading terminaltables-3.1.10-py2.py3-none-any.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from sahi) (2.32.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from sahi) (8.1.7)\n",
            "Requirement already satisfied: numpy<2.0.0 in /usr/local/lib/python3.10/dist-packages (from sahi) (1.26.4)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from fire->sahi) (2.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->sahi) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->sahi) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->sahi) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->sahi) (2024.8.30)\n",
            "Downloading sahi-0.11.19-py3-none-any.whl (111 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m111.3/111.3 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pybboxes-0.1.6-py3-none-any.whl (24 kB)\n",
            "Downloading opencv_python-4.9.0.80-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (62.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.2/62.2 MB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading terminaltables-3.1.10-py2.py3-none-any.whl (15 kB)\n",
            "Building wheels for collected packages: fire\n",
            "  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fire: filename=fire-0.7.0-py3-none-any.whl size=114249 sha256=c650f7b3636bde47035a9bec9c9721ca246466b4a8f1b0b9e4a7296fb85ae93f\n",
            "  Stored in directory: /root/.cache/pip/wheels/19/39/2f/2d3cadc408a8804103f1c34ddd4b9f6a93497b11fa96fe738e\n",
            "Successfully built fire\n",
            "Installing collected packages: terminaltables, pybboxes, opencv-python, fire, sahi\n",
            "  Attempting uninstall: opencv-python\n",
            "    Found existing installation: opencv-python 4.10.0.84\n",
            "    Uninstalling opencv-python-4.10.0.84:\n",
            "      Successfully uninstalled opencv-python-4.10.0.84\n",
            "Successfully installed fire-0.7.0 opencv-python-4.9.0.80 pybboxes-0.1.6 sahi-0.11.19 terminaltables-3.1.10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yvV46laZ2mox",
        "outputId": "fbff02cc-02ab-46a9-a255-702fb048d6d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ultralytics\n",
            "  Downloading ultralytics-8.3.35-py3-none-any.whl.metadata (35 kB)\n",
            "Requirement already satisfied: numpy>=1.23.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.26.4)\n",
            "Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (3.8.0)\n",
            "Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.9.0.80)\n",
            "Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (11.0.0)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (6.0.2)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.32.3)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.13.1)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.5.1+cu121)\n",
            "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.20.1+cu121)\n",
            "Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.66.6)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from ultralytics) (5.9.5)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from ultralytics) (9.0.0)\n",
            "Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.2.2)\n",
            "Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.13.2)\n",
            "Collecting ultralytics-thop>=2.0.0 (from ultralytics)\n",
            "  Downloading ultralytics_thop-2.0.12-py3-none-any.whl.metadata (9.4 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.55.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (24.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics) (2024.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2024.8.30)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.8.0->ultralytics) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.2)\n",
            "Downloading ultralytics-8.3.35-py3-none-any.whl (887 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m887.4/887.4 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ultralytics_thop-2.0.12-py3-none-any.whl (26 kB)\n",
            "Installing collected packages: ultralytics-thop, ultralytics\n",
            "Successfully installed ultralytics-8.3.35 ultralytics-thop-2.0.12\n"
          ]
        }
      ],
      "source": [
        "!pip install ultralytics"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install roboflow\n",
        "\n",
        "from roboflow import Roboflow\n",
        "rf = Roboflow(api_key=\"",
        "project = rf.workspace(\"taj-90dsy\").project(\"football-detection-msim6\")\n",
        "version = project.version(7)\n",
        "dataset = version.download(\"yolov8\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vBmPf5Fy29Ib",
        "outputId": "933cc438-28f0-4a35-ab56-1313b8d6dff2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting roboflow\n",
            "  Downloading roboflow-1.1.49-py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from roboflow) (2024.8.30)\n",
            "Collecting idna==3.7 (from roboflow)\n",
            "  Downloading idna-3.7-py3-none-any.whl.metadata (9.9 kB)\n",
            "Requirement already satisfied: cycler in /usr/local/lib/python3.10/dist-packages (from roboflow) (0.12.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.10/dist-packages (from roboflow) (1.4.7)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from roboflow) (3.8.0)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from roboflow) (1.26.4)\n",
            "Requirement already satisfied: opencv-python-headless==4.10.0.84 in /usr/local/lib/python3.10/dist-packages (from roboflow) (4.10.0.84)\n",
            "Requirement already satisfied: Pillow>=7.1.2 in /usr/local/lib/python3.10/dist-packages (from roboflow) (11.0.0)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from roboflow) (2.8.2)\n",
            "Collecting python-dotenv (from roboflow)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from roboflow) (2.32.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from roboflow) (1.16.0)\n",
            "Requirement already satisfied: urllib3>=1.26.6 in /usr/local/lib/python3.10/dist-packages (from roboflow) (2.2.3)\n",
            "Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.10/dist-packages (from roboflow) (4.66.6)\n",
            "Requirement already satisfied: PyYAML>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from roboflow) (6.0.2)\n",
            "Requirement already satisfied: requests-toolbelt in /usr/local/lib/python3.10/dist-packages (from roboflow) (1.0.0)\n",
            "Collecting filetype (from roboflow)\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->roboflow) (1.3.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->roboflow) (4.55.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->roboflow) (24.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->roboflow) (3.2.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->roboflow) (3.4.0)\n",
            "Downloading roboflow-1.1.49-py3-none-any.whl (80 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m80.9/80.9 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading idna-3.7-py3-none-any.whl (66 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.8/66.8 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Installing collected packages: filetype, python-dotenv, idna, roboflow\n",
            "  Attempting uninstall: idna\n",
            "    Found existing installation: idna 3.10\n",
            "    Uninstalling idna-3.10:\n",
            "      Successfully uninstalled idna-3.10\n",
            "Successfully installed filetype-1.2.0 idna-3.7 python-dotenv-1.0.1 roboflow-1.1.49\n",
            "loading Roboflow workspace...\n",
            "loading Roboflow project...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading Dataset Version Zip in Football-Detection-7 to yolov8:: 100%|██████████| 232409/232409 [00:04<00:00, 53847.59it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Extracting Dataset Version Zip to Football-Detection-7 in yolov8:: 100%|██████████| 7817/7817 [00:05<00:00, 1505.36it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating new Ultralytics Settings v0.0.6 file ✅ \n",
            "View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n",
            "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/ultralytics/ultralytics.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vcM9MKCv-OZ_",
        "outputId": "a55f43a0-6c34-4177-a718-6311bbf3080f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'ultralytics'...\n",
            "remote: Enumerating objects: 46292, done.\u001b[K\n",
            "remote: Counting objects: 100% (485/485), done.\u001b[K\n",
            "remote: Compressing objects: 100% (346/346), done.\u001b[K\n",
            "remote: Total 46292 (delta 292), reused 265 (delta 138), pack-reused 45807 (from 1)\u001b[K\n",
            "Receiving objects: 100% (46292/46292), 38.48 MiB | 26.55 MiB/s, done.\n",
            "Resolving deltas: 100% (34448/34448), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from ultralytics import YOLO"
      ],
      "metadata": {
        "id": "sXk0Scvc3rqy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = YOLO(\"yolov8n.pt\")"
      ],
      "metadata": {
        "id": "hrmDvUqh35Mx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf90485b-ab12-4c92-8874-785fa5632dea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolov8n.pt to 'yolov8n.pt'...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 6.25M/6.25M [00:00<00:00, 126MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn"
      ],
      "metadata": {
        "id": "sgIkmcSh37eF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "original_head = model.model.model[-1]"
      ],
      "metadata": {
        "id": "c_fabher39YC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomDetectionHead(nn.Module):\n",
        "    def __init__(self, in_channels, num_classes):\n",
        "        super(CustomDetectionHead, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, in_channels, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(in_channels, in_channels, kernel_size=3, padding=1)\n",
        "        self.predict = nn.Conv2d(in_channels, num_classes + 5, kernel_size=1)  # 5 for bbox coords and confidence\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        return self.predict(x)"
      ],
      "metadata": {
        "id": "8Tx-fV5w4AW5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_head = CustomDetectionHead(in_channels=80, num_classes=1)"
      ],
      "metadata": {
        "id": "_1FLKfXz4Bea"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.model.model[-1] = nn.Sequential(\n",
        "    original_head,  # Existing detection head\n",
        "    new_head        # New detection head\n",
        ")"
      ],
      "metadata": {
        "id": "iSHYB8An4MiX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "import torch\n",
        "\n",
        "gc.collect()\n",
        "\n",
        "# Free GPU memory (specific to PyTorch)\n",
        "torch.cuda.empty_cache()\n",
        "torch.cuda.empty_cache()\n",
        "torch.cuda.empty_cache()\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "PpOUUeqDLi1z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.train(\n",
        "    data='/content/Football-Detection-7/data.yaml',  # Dataset configuration\n",
        "    epochs=20,\n",
        "    batch=12,\n",
        "    imgsz=1280\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dwHXPZzl4PYn",
        "outputId": "21e3217b-a106-4dcd-93b4-16d0d74b475f"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ultralytics 8.3.34 🚀 Python-3.10.12 torch-2.5.1+cu121 CUDA:0 (Tesla T4, 15102MiB)\n",
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolov8n.pt, data=/content/Football-Detection-7/data.yaml, epochs=20, time=None, patience=100, batch=12, imgsz=1280, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=train5, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs/detect/train5\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
            "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
            "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
            "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
            "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
            "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
            "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
            "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
            "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
            "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
            " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
            " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
            " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
            " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
            " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
            " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
            " 22        [15, 18, 21]  1    751507  ultralytics.nn.modules.head.Detect           [1, [64, 128, 256]]           \n",
            "Model summary: 225 layers, 3,011,043 parameters, 3,011,027 gradients, 8.2 GFLOPs\n",
            "\n",
            "Transferred 355/355 items from pretrained weights\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/detect/train5', view at http://localhost:6006/\n",
            "Freezing layer 'model.22.dfl.conv.weight'\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/Football-Detection-7/train/labels.cache... 3644 images, 2726 backgrounds, 0 corrupt: 100%|██████████| 3644/3644 [00:00<?, ?it/s]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, num_output_channels=3, method='weighted_average'), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/Football-Detection-7/valid/labels.cache... 260 images, 190 backgrounds, 0 corrupt: 100%|██████████| 260/260 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Plotting labels to runs/detect/train5/labels.jpg... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.002, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.00046875), 63 bias(decay=0.0)\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added ✅\n",
            "Image sizes 1280 train, 1280 val\n",
            "Using 2 dataloader workers\n",
            "Logging results to \u001b[1mruns/detect/train5\u001b[0m\n",
            "Starting training for 20 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "       1/20      6.55G      1.949      27.08      1.119          3       1280: 100%|██████████| 304/304 [04:47<00:00,  1.06it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 11/11 [00:06<00:00,  1.59it/s]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        260         70     0.0293      0.471     0.0169     0.0082\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "       2/20      6.15G      1.906      8.368      1.037          1       1280: 100%|██████████| 304/304 [04:47<00:00,  1.06it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 11/11 [00:06<00:00,  1.76it/s]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        260         70      0.663      0.557      0.614      0.263\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "       3/20      6.19G      1.687      2.438     0.9765          5       1280: 100%|██████████| 304/304 [04:50<00:00,  1.05it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 11/11 [00:06<00:00,  1.66it/s]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        260         70      0.929      0.557       0.65      0.367\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "       4/20      6.16G      1.616      1.693     0.9563          5       1280: 100%|██████████| 304/304 [05:10<00:00,  1.02s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 11/11 [00:06<00:00,  1.69it/s]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        260         70      0.924      0.522      0.644      0.359\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "       5/20      6.16G      1.604      1.304     0.9514          1       1280: 100%|██████████| 304/304 [04:54<00:00,  1.03it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 11/11 [00:07<00:00,  1.51it/s]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        260         70      0.976      0.585      0.698      0.444\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "       6/20      6.15G      1.547      1.247     0.9259          2       1280: 100%|██████████| 304/304 [05:30<00:00,  1.09s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 11/11 [00:04<00:00,  2.40it/s]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        260         70      0.856      0.595       0.69      0.407\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "       7/20      6.15G      1.485      1.041     0.9188          5       1280: 100%|██████████| 304/304 [05:18<00:00,  1.05s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 11/11 [00:05<00:00,  2.04it/s]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        260         70      0.845      0.629      0.701      0.382\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       8/20      6.15G      1.503     0.9811     0.9097          4       1280: 100%|██████████| 304/304 [06:21<00:00,  1.26s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 11/11 [00:04<00:00,  2.45it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        260         70      0.955      0.607      0.754      0.452\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       9/20      6.15G      1.426      0.944     0.9077          5       1280: 100%|██████████| 304/304 [05:52<00:00,  1.16s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 11/11 [00:04<00:00,  2.45it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        260         70        0.8      0.629      0.699      0.427\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      10/20      6.15G      1.401     0.8048     0.9096          1       1280: 100%|██████████| 304/304 [05:34<00:00,  1.10s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 11/11 [00:05<00:00,  1.92it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        260         70       0.86      0.657      0.736      0.394\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Closing dataloader mosaic\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, num_output_channels=3, method='weighted_average'), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      11/20      6.67G      1.277     0.9179     0.8416          3       1280: 100%|██████████| 304/304 [03:59<00:00,  1.27it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 11/11 [00:04<00:00,  2.48it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        260         70      0.904      0.586      0.708      0.405\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      12/20      6.13G      1.306     0.8394     0.8643          2       1280: 100%|██████████| 304/304 [03:59<00:00,  1.27it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 11/11 [00:06<00:00,  1.79it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        260         70      0.977      0.614      0.792      0.488\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      13/20      6.13G      1.281     0.7792     0.8484          1       1280: 100%|██████████| 304/304 [04:16<00:00,  1.18it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 11/11 [00:04<00:00,  2.53it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        260         70       0.96      0.643      0.771       0.47\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      14/20      6.13G      1.283     0.7404     0.8509          1       1280: 100%|██████████| 304/304 [04:14<00:00,  1.19it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 11/11 [00:05<00:00,  2.11it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        260         70      0.872      0.671      0.781      0.484\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      15/20      6.13G      1.223     0.7135     0.8324          3       1280: 100%|██████████| 304/304 [04:21<00:00,  1.16it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 11/11 [00:06<00:00,  1.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        260         70       0.92      0.671      0.767      0.477\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      16/20      6.13G      1.289     0.7402     0.8366          2       1280: 100%|██████████| 304/304 [04:08<00:00,  1.22it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 11/11 [00:07<00:00,  1.42it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        260         70      0.947        0.6      0.731      0.438\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      17/20      6.13G      1.263     0.7374     0.8589          1       1280: 100%|██████████| 304/304 [04:20<00:00,  1.17it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 11/11 [00:06<00:00,  1.69it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        260         70      0.997      0.614      0.789      0.497\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      18/20      6.13G      1.235     0.6929     0.8495          3       1280: 100%|██████████| 304/304 [04:13<00:00,  1.20it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 11/11 [00:05<00:00,  1.89it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        260         70      0.957      0.637      0.767      0.484\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      19/20      6.13G      1.144     0.6227     0.8201          1       1280: 100%|██████████| 304/304 [04:18<00:00,  1.18it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 11/11 [00:07<00:00,  1.47it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        260         70      0.939      0.665      0.754      0.497\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      20/20      6.13G      1.162     0.6122      0.833          2       1280: 100%|██████████| 304/304 [04:11<00:00,  1.21it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 11/11 [00:07<00:00,  1.50it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        260         70      0.937      0.671      0.757      0.493\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "20 epochs completed in 1.634 hours.\n",
            "Optimizer stripped from runs/detect/train5/weights/last.pt, 6.3MB\n",
            "Optimizer stripped from runs/detect/train5/weights/best.pt, 6.3MB\n",
            "\n",
            "Validating runs/detect/train5/weights/best.pt...\n",
            "Ultralytics 8.3.34 🚀 Python-3.10.12 torch-2.5.1+cu121 CUDA:0 (Tesla T4, 15102MiB)\n",
            "Model summary (fused): 168 layers, 3,005,843 parameters, 0 gradients, 8.1 GFLOPs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 11/11 [00:13<00:00,  1.24s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        260         70      0.997      0.614       0.79      0.498\n",
            "Speed: 1.9ms preprocess, 21.3ms inference, 0.0ms loss, 4.1ms postprocess per image\n",
            "Results saved to \u001b[1mruns/detect/train5\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ultralytics.utils.metrics.DetMetrics object with attributes:\n",
              "\n",
              "ap_class_index: array([0])\n",
              "box: ultralytics.utils.metrics.Metric object\n",
              "confusion_matrix: <ultralytics.utils.metrics.ConfusionMatrix object at 0x7b0a66a97490>\n",
              "curves: ['Precision-Recall(B)', 'F1-Confidence(B)', 'Precision-Confidence(B)', 'Recall-Confidence(B)']\n",
              "curves_results: [[array([          0,    0.001001,    0.002002,    0.003003,    0.004004,    0.005005,    0.006006,    0.007007,    0.008008,    0.009009,     0.01001,    0.011011,    0.012012,    0.013013,    0.014014,    0.015015,    0.016016,    0.017017,    0.018018,    0.019019,     0.02002,    0.021021,    0.022022,    0.023023,\n",
              "          0.024024,    0.025025,    0.026026,    0.027027,    0.028028,    0.029029,     0.03003,    0.031031,    0.032032,    0.033033,    0.034034,    0.035035,    0.036036,    0.037037,    0.038038,    0.039039,     0.04004,    0.041041,    0.042042,    0.043043,    0.044044,    0.045045,    0.046046,    0.047047,\n",
              "          0.048048,    0.049049,     0.05005,    0.051051,    0.052052,    0.053053,    0.054054,    0.055055,    0.056056,    0.057057,    0.058058,    0.059059,     0.06006,    0.061061,    0.062062,    0.063063,    0.064064,    0.065065,    0.066066,    0.067067,    0.068068,    0.069069,     0.07007,    0.071071,\n",
              "          0.072072,    0.073073,    0.074074,    0.075075,    0.076076,    0.077077,    0.078078,    0.079079,     0.08008,    0.081081,    0.082082,    0.083083,    0.084084,    0.085085,    0.086086,    0.087087,    0.088088,    0.089089,     0.09009,    0.091091,    0.092092,    0.093093,    0.094094,    0.095095,\n",
              "          0.096096,    0.097097,    0.098098,    0.099099,      0.1001,      0.1011,      0.1021,      0.1031,      0.1041,     0.10511,     0.10611,     0.10711,     0.10811,     0.10911,     0.11011,     0.11111,     0.11211,     0.11311,     0.11411,     0.11512,     0.11612,     0.11712,     0.11812,     0.11912,\n",
              "           0.12012,     0.12112,     0.12212,     0.12312,     0.12412,     0.12513,     0.12613,     0.12713,     0.12813,     0.12913,     0.13013,     0.13113,     0.13213,     0.13313,     0.13413,     0.13514,     0.13614,     0.13714,     0.13814,     0.13914,     0.14014,     0.14114,     0.14214,     0.14314,\n",
              "           0.14414,     0.14515,     0.14615,     0.14715,     0.14815,     0.14915,     0.15015,     0.15115,     0.15215,     0.15315,     0.15415,     0.15516,     0.15616,     0.15716,     0.15816,     0.15916,     0.16016,     0.16116,     0.16216,     0.16316,     0.16416,     0.16517,     0.16617,     0.16717,\n",
              "           0.16817,     0.16917,     0.17017,     0.17117,     0.17217,     0.17317,     0.17417,     0.17518,     0.17618,     0.17718,     0.17818,     0.17918,     0.18018,     0.18118,     0.18218,     0.18318,     0.18418,     0.18519,     0.18619,     0.18719,     0.18819,     0.18919,     0.19019,     0.19119,\n",
              "           0.19219,     0.19319,     0.19419,      0.1952,      0.1962,      0.1972,      0.1982,      0.1992,      0.2002,      0.2012,      0.2022,      0.2032,      0.2042,     0.20521,     0.20621,     0.20721,     0.20821,     0.20921,     0.21021,     0.21121,     0.21221,     0.21321,     0.21421,     0.21522,\n",
              "           0.21622,     0.21722,     0.21822,     0.21922,     0.22022,     0.22122,     0.22222,     0.22322,     0.22422,     0.22523,     0.22623,     0.22723,     0.22823,     0.22923,     0.23023,     0.23123,     0.23223,     0.23323,     0.23423,     0.23524,     0.23624,     0.23724,     0.23824,     0.23924,\n",
              "           0.24024,     0.24124,     0.24224,     0.24324,     0.24424,     0.24525,     0.24625,     0.24725,     0.24825,     0.24925,     0.25025,     0.25125,     0.25225,     0.25325,     0.25425,     0.25526,     0.25626,     0.25726,     0.25826,     0.25926,     0.26026,     0.26126,     0.26226,     0.26326,\n",
              "           0.26426,     0.26527,     0.26627,     0.26727,     0.26827,     0.26927,     0.27027,     0.27127,     0.27227,     0.27327,     0.27427,     0.27528,     0.27628,     0.27728,     0.27828,     0.27928,     0.28028,     0.28128,     0.28228,     0.28328,     0.28428,     0.28529,     0.28629,     0.28729,\n",
              "           0.28829,     0.28929,     0.29029,     0.29129,     0.29229,     0.29329,     0.29429,      0.2953,      0.2963,      0.2973,      0.2983,      0.2993,      0.3003,      0.3013,      0.3023,      0.3033,      0.3043,     0.30531,     0.30631,     0.30731,     0.30831,     0.30931,     0.31031,     0.31131,\n",
              "           0.31231,     0.31331,     0.31431,     0.31532,     0.31632,     0.31732,     0.31832,     0.31932,     0.32032,     0.32132,     0.32232,     0.32332,     0.32432,     0.32533,     0.32633,     0.32733,     0.32833,     0.32933,     0.33033,     0.33133,     0.33233,     0.33333,     0.33433,     0.33534,\n",
              "           0.33634,     0.33734,     0.33834,     0.33934,     0.34034,     0.34134,     0.34234,     0.34334,     0.34434,     0.34535,     0.34635,     0.34735,     0.34835,     0.34935,     0.35035,     0.35135,     0.35235,     0.35335,     0.35435,     0.35536,     0.35636,     0.35736,     0.35836,     0.35936,\n",
              "           0.36036,     0.36136,     0.36236,     0.36336,     0.36436,     0.36537,     0.36637,     0.36737,     0.36837,     0.36937,     0.37037,     0.37137,     0.37237,     0.37337,     0.37437,     0.37538,     0.37638,     0.37738,     0.37838,     0.37938,     0.38038,     0.38138,     0.38238,     0.38338,\n",
              "           0.38438,     0.38539,     0.38639,     0.38739,     0.38839,     0.38939,     0.39039,     0.39139,     0.39239,     0.39339,     0.39439,      0.3954,      0.3964,      0.3974,      0.3984,      0.3994,      0.4004,      0.4014,      0.4024,      0.4034,      0.4044,     0.40541,     0.40641,     0.40741,\n",
              "           0.40841,     0.40941,     0.41041,     0.41141,     0.41241,     0.41341,     0.41441,     0.41542,     0.41642,     0.41742,     0.41842,     0.41942,     0.42042,     0.42142,     0.42242,     0.42342,     0.42442,     0.42543,     0.42643,     0.42743,     0.42843,     0.42943,     0.43043,     0.43143,\n",
              "           0.43243,     0.43343,     0.43443,     0.43544,     0.43644,     0.43744,     0.43844,     0.43944,     0.44044,     0.44144,     0.44244,     0.44344,     0.44444,     0.44545,     0.44645,     0.44745,     0.44845,     0.44945,     0.45045,     0.45145,     0.45245,     0.45345,     0.45445,     0.45546,\n",
              "           0.45646,     0.45746,     0.45846,     0.45946,     0.46046,     0.46146,     0.46246,     0.46346,     0.46446,     0.46547,     0.46647,     0.46747,     0.46847,     0.46947,     0.47047,     0.47147,     0.47247,     0.47347,     0.47447,     0.47548,     0.47648,     0.47748,     0.47848,     0.47948,\n",
              "           0.48048,     0.48148,     0.48248,     0.48348,     0.48448,     0.48549,     0.48649,     0.48749,     0.48849,     0.48949,     0.49049,     0.49149,     0.49249,     0.49349,     0.49449,      0.4955,      0.4965,      0.4975,      0.4985,      0.4995,      0.5005,      0.5015,      0.5025,      0.5035,\n",
              "            0.5045,     0.50551,     0.50651,     0.50751,     0.50851,     0.50951,     0.51051,     0.51151,     0.51251,     0.51351,     0.51451,     0.51552,     0.51652,     0.51752,     0.51852,     0.51952,     0.52052,     0.52152,     0.52252,     0.52352,     0.52452,     0.52553,     0.52653,     0.52753,\n",
              "           0.52853,     0.52953,     0.53053,     0.53153,     0.53253,     0.53353,     0.53453,     0.53554,     0.53654,     0.53754,     0.53854,     0.53954,     0.54054,     0.54154,     0.54254,     0.54354,     0.54454,     0.54555,     0.54655,     0.54755,     0.54855,     0.54955,     0.55055,     0.55155,\n",
              "           0.55255,     0.55355,     0.55455,     0.55556,     0.55656,     0.55756,     0.55856,     0.55956,     0.56056,     0.56156,     0.56256,     0.56356,     0.56456,     0.56557,     0.56657,     0.56757,     0.56857,     0.56957,     0.57057,     0.57157,     0.57257,     0.57357,     0.57457,     0.57558,\n",
              "           0.57658,     0.57758,     0.57858,     0.57958,     0.58058,     0.58158,     0.58258,     0.58358,     0.58458,     0.58559,     0.58659,     0.58759,     0.58859,     0.58959,     0.59059,     0.59159,     0.59259,     0.59359,     0.59459,      0.5956,      0.5966,      0.5976,      0.5986,      0.5996,\n",
              "            0.6006,      0.6016,      0.6026,      0.6036,      0.6046,     0.60561,     0.60661,     0.60761,     0.60861,     0.60961,     0.61061,     0.61161,     0.61261,     0.61361,     0.61461,     0.61562,     0.61662,     0.61762,     0.61862,     0.61962,     0.62062,     0.62162,     0.62262,     0.62362,\n",
              "           0.62462,     0.62563,     0.62663,     0.62763,     0.62863,     0.62963,     0.63063,     0.63163,     0.63263,     0.63363,     0.63463,     0.63564,     0.63664,     0.63764,     0.63864,     0.63964,     0.64064,     0.64164,     0.64264,     0.64364,     0.64464,     0.64565,     0.64665,     0.64765,\n",
              "           0.64865,     0.64965,     0.65065,     0.65165,     0.65265,     0.65365,     0.65465,     0.65566,     0.65666,     0.65766,     0.65866,     0.65966,     0.66066,     0.66166,     0.66266,     0.66366,     0.66466,     0.66567,     0.66667,     0.66767,     0.66867,     0.66967,     0.67067,     0.67167,\n",
              "           0.67267,     0.67367,     0.67467,     0.67568,     0.67668,     0.67768,     0.67868,     0.67968,     0.68068,     0.68168,     0.68268,     0.68368,     0.68468,     0.68569,     0.68669,     0.68769,     0.68869,     0.68969,     0.69069,     0.69169,     0.69269,     0.69369,     0.69469,      0.6957,\n",
              "            0.6967,      0.6977,      0.6987,      0.6997,      0.7007,      0.7017,      0.7027,      0.7037,      0.7047,     0.70571,     0.70671,     0.70771,     0.70871,     0.70971,     0.71071,     0.71171,     0.71271,     0.71371,     0.71471,     0.71572,     0.71672,     0.71772,     0.71872,     0.71972,\n",
              "           0.72072,     0.72172,     0.72272,     0.72372,     0.72472,     0.72573,     0.72673,     0.72773,     0.72873,     0.72973,     0.73073,     0.73173,     0.73273,     0.73373,     0.73473,     0.73574,     0.73674,     0.73774,     0.73874,     0.73974,     0.74074,     0.74174,     0.74274,     0.74374,\n",
              "           0.74474,     0.74575,     0.74675,     0.74775,     0.74875,     0.74975,     0.75075,     0.75175,     0.75275,     0.75375,     0.75475,     0.75576,     0.75676,     0.75776,     0.75876,     0.75976,     0.76076,     0.76176,     0.76276,     0.76376,     0.76476,     0.76577,     0.76677,     0.76777,\n",
              "           0.76877,     0.76977,     0.77077,     0.77177,     0.77277,     0.77377,     0.77477,     0.77578,     0.77678,     0.77778,     0.77878,     0.77978,     0.78078,     0.78178,     0.78278,     0.78378,     0.78478,     0.78579,     0.78679,     0.78779,     0.78879,     0.78979,     0.79079,     0.79179,\n",
              "           0.79279,     0.79379,     0.79479,      0.7958,      0.7968,      0.7978,      0.7988,      0.7998,      0.8008,      0.8018,      0.8028,      0.8038,      0.8048,     0.80581,     0.80681,     0.80781,     0.80881,     0.80981,     0.81081,     0.81181,     0.81281,     0.81381,     0.81481,     0.81582,\n",
              "           0.81682,     0.81782,     0.81882,     0.81982,     0.82082,     0.82182,     0.82282,     0.82382,     0.82482,     0.82583,     0.82683,     0.82783,     0.82883,     0.82983,     0.83083,     0.83183,     0.83283,     0.83383,     0.83483,     0.83584,     0.83684,     0.83784,     0.83884,     0.83984,\n",
              "           0.84084,     0.84184,     0.84284,     0.84384,     0.84484,     0.84585,     0.84685,     0.84785,     0.84885,     0.84985,     0.85085,     0.85185,     0.85285,     0.85385,     0.85485,     0.85586,     0.85686,     0.85786,     0.85886,     0.85986,     0.86086,     0.86186,     0.86286,     0.86386,\n",
              "           0.86486,     0.86587,     0.86687,     0.86787,     0.86887,     0.86987,     0.87087,     0.87187,     0.87287,     0.87387,     0.87487,     0.87588,     0.87688,     0.87788,     0.87888,     0.87988,     0.88088,     0.88188,     0.88288,     0.88388,     0.88488,     0.88589,     0.88689,     0.88789,\n",
              "           0.88889,     0.88989,     0.89089,     0.89189,     0.89289,     0.89389,     0.89489,      0.8959,      0.8969,      0.8979,      0.8989,      0.8999,      0.9009,      0.9019,      0.9029,      0.9039,      0.9049,     0.90591,     0.90691,     0.90791,     0.90891,     0.90991,     0.91091,     0.91191,\n",
              "           0.91291,     0.91391,     0.91491,     0.91592,     0.91692,     0.91792,     0.91892,     0.91992,     0.92092,     0.92192,     0.92292,     0.92392,     0.92492,     0.92593,     0.92693,     0.92793,     0.92893,     0.92993,     0.93093,     0.93193,     0.93293,     0.93393,     0.93493,     0.93594,\n",
              "           0.93694,     0.93794,     0.93894,     0.93994,     0.94094,     0.94194,     0.94294,     0.94394,     0.94494,     0.94595,     0.94695,     0.94795,     0.94895,     0.94995,     0.95095,     0.95195,     0.95295,     0.95395,     0.95495,     0.95596,     0.95696,     0.95796,     0.95896,     0.95996,\n",
              "           0.96096,     0.96196,     0.96296,     0.96396,     0.96496,     0.96597,     0.96697,     0.96797,     0.96897,     0.96997,     0.97097,     0.97197,     0.97297,     0.97397,     0.97497,     0.97598,     0.97698,     0.97798,     0.97898,     0.97998,     0.98098,     0.98198,     0.98298,     0.98398,\n",
              "           0.98498,     0.98599,     0.98699,     0.98799,     0.98899,     0.98999,     0.99099,     0.99199,     0.99299,     0.99399,     0.99499,       0.996,       0.997,       0.998,       0.999,           1]), array([[          1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
              "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
              "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
              "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
              "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
              "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
              "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
              "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
              "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
              "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
              "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
              "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
              "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
              "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
              "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
              "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
              "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
              "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
              "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
              "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
              "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
              "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
              "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
              "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
              "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
              "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
              "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,        0.88,        0.88,        0.88,        0.88,        0.88,        0.88,        0.88,\n",
              "               0.88,        0.88,        0.88,        0.88,        0.88,        0.88,        0.88,     0.86538,     0.86538,     0.86538,     0.86538,     0.86538,     0.86538,     0.86538,     0.86538,     0.86538,     0.86538,     0.86538,     0.86538,     0.86538,     0.86538,     0.86538,     0.85185,\n",
              "            0.85185,     0.85185,     0.85185,     0.85185,     0.85185,     0.85185,     0.85185,     0.85185,     0.85185,     0.85185,     0.85185,     0.85185,     0.85185,     0.84211,     0.84211,     0.84211,     0.84211,     0.84211,     0.84211,     0.84211,     0.84211,     0.84211,     0.84211,\n",
              "            0.84211,     0.84211,     0.84211,     0.84211,     0.84211,     0.84211,     0.84211,     0.84211,     0.84211,     0.84211,     0.84211,     0.84211,     0.84211,     0.84211,     0.84211,     0.84211,     0.84211,     0.84211,     0.84211,     0.83333,     0.83333,     0.83333,     0.83333,\n",
              "            0.83333,     0.83333,     0.83333,     0.83333,     0.83333,     0.83333,     0.83333,     0.83333,     0.83333,     0.83333,     0.83333,     0.83333,     0.83333,     0.83333,     0.83333,     0.83333,     0.83333,     0.83333,     0.83333,     0.83333,     0.83333,     0.83333,     0.83333,\n",
              "            0.83333,     0.82258,     0.82258,     0.82258,     0.82258,     0.82258,     0.82258,     0.82258,     0.82258,     0.82258,     0.82258,     0.82258,     0.82258,     0.82258,     0.82258,     0.77612,     0.77612,     0.77612,     0.77612,     0.77612,     0.77612,     0.77612,     0.77612,\n",
              "            0.77612,     0.77612,     0.77612,     0.77612,     0.77612,     0.77612,     0.77612,     0.75714,     0.75714,     0.75714,     0.75714,     0.75714,     0.75714,     0.75714,     0.75714,     0.75714,     0.75714,     0.75714,     0.75714,     0.75714,     0.75714,      0.6506,      0.6506,\n",
              "             0.6506,      0.6506,      0.6506,      0.6506,      0.6506,      0.6506,      0.6506,      0.6506,      0.6506,      0.6506,      0.6506,      0.6506,     0.61798,     0.61798,     0.61798,     0.61798,     0.61798,     0.61798,     0.61798,     0.61798,     0.61798,     0.61798,     0.61798,\n",
              "            0.61798,     0.61798,     0.61798,         0.5,         0.5,         0.5,         0.5,         0.5,         0.5,         0.5,         0.5,         0.5,         0.5,         0.5,         0.5,         0.5,         0.5,         0.5,         0.5,         0.5,         0.5,         0.5,         0.5,\n",
              "                0.5,         0.5,         0.5,         0.5,         0.5,         0.5,         0.5,         0.5,         0.5,     0.45669,     0.45669,     0.45669,     0.45669,     0.45669,     0.45669,     0.45669,     0.45669,     0.45669,     0.45669,     0.45669,     0.45669,     0.45669,     0.45669,\n",
              "             0.2823,      0.2823,      0.2823,      0.2823,      0.2823,      0.2823,      0.2823,      0.2823,      0.2823,      0.2823,      0.2823,      0.2823,      0.2823,      0.2823,      0.2823,     0.18208,     0.18091,     0.17974,     0.17858,     0.17741,     0.17624,     0.17508,     0.17391,\n",
              "            0.17274,     0.17157,     0.17041,     0.16924,     0.16807,     0.16691,     0.16574,     0.16457,      0.1634,     0.16224,     0.16107,      0.1599,     0.15874,     0.15757,      0.1564,     0.15523,     0.15407,      0.1529,     0.15173,     0.15057,      0.1494,     0.14823,     0.14706,\n",
              "             0.1459,     0.14473,     0.14356,      0.1424,     0.14123,     0.14006,     0.13889,     0.13773,     0.13656,     0.13539,     0.13423,     0.13306,     0.13189,     0.13072,     0.12956,     0.12839,     0.12722,     0.12605,     0.12489,     0.12372,     0.12255,     0.12139,     0.12022,\n",
              "            0.11905,     0.11788,     0.11672,     0.11555,     0.11438,     0.11322,     0.11205,     0.11088,     0.10971,     0.10855,     0.10738,     0.10621,     0.10505,     0.10388,     0.10271,     0.10154,     0.10038,     0.09921,    0.098043,    0.096876,    0.095708,    0.094541,    0.093374,\n",
              "           0.092207,     0.09104,    0.089872,    0.088705,    0.087538,    0.086371,    0.085204,    0.084037,    0.082869,    0.081702,    0.080535,    0.079368,    0.078201,    0.077034,    0.075866,    0.074699,    0.073532,    0.072365,    0.071198,    0.070031,    0.068863,    0.067696,    0.066529,\n",
              "           0.065362,    0.064195,    0.063027,     0.06186,    0.060693,    0.059526,    0.058359,    0.057192,    0.056024,    0.054857,     0.05369,    0.052523,    0.051356,    0.050189,    0.049021,    0.047854,    0.046687,     0.04552,    0.044353,    0.043185,    0.042018,    0.040851,    0.039684,\n",
              "           0.038517,     0.03735,    0.036182,    0.035015,    0.033848,    0.032681,    0.031514,    0.030347,    0.029179,    0.028012,    0.026845,    0.025678,    0.024511,    0.023344,    0.022176,    0.021009,    0.019842,    0.018675,    0.017508,     0.01634,    0.015173,    0.014006,    0.012839,\n",
              "           0.011672,    0.010505,   0.0093374,   0.0081702,   0.0070031,   0.0058359,   0.0046687,   0.0035015,   0.0023344,   0.0011672,           0]]), 'Recall', 'Precision'], [array([          0,    0.001001,    0.002002,    0.003003,    0.004004,    0.005005,    0.006006,    0.007007,    0.008008,    0.009009,     0.01001,    0.011011,    0.012012,    0.013013,    0.014014,    0.015015,    0.016016,    0.017017,    0.018018,    0.019019,     0.02002,    0.021021,    0.022022,    0.023023,\n",
              "          0.024024,    0.025025,    0.026026,    0.027027,    0.028028,    0.029029,     0.03003,    0.031031,    0.032032,    0.033033,    0.034034,    0.035035,    0.036036,    0.037037,    0.038038,    0.039039,     0.04004,    0.041041,    0.042042,    0.043043,    0.044044,    0.045045,    0.046046,    0.047047,\n",
              "          0.048048,    0.049049,     0.05005,    0.051051,    0.052052,    0.053053,    0.054054,    0.055055,    0.056056,    0.057057,    0.058058,    0.059059,     0.06006,    0.061061,    0.062062,    0.063063,    0.064064,    0.065065,    0.066066,    0.067067,    0.068068,    0.069069,     0.07007,    0.071071,\n",
              "          0.072072,    0.073073,    0.074074,    0.075075,    0.076076,    0.077077,    0.078078,    0.079079,     0.08008,    0.081081,    0.082082,    0.083083,    0.084084,    0.085085,    0.086086,    0.087087,    0.088088,    0.089089,     0.09009,    0.091091,    0.092092,    0.093093,    0.094094,    0.095095,\n",
              "          0.096096,    0.097097,    0.098098,    0.099099,      0.1001,      0.1011,      0.1021,      0.1031,      0.1041,     0.10511,     0.10611,     0.10711,     0.10811,     0.10911,     0.11011,     0.11111,     0.11211,     0.11311,     0.11411,     0.11512,     0.11612,     0.11712,     0.11812,     0.11912,\n",
              "           0.12012,     0.12112,     0.12212,     0.12312,     0.12412,     0.12513,     0.12613,     0.12713,     0.12813,     0.12913,     0.13013,     0.13113,     0.13213,     0.13313,     0.13413,     0.13514,     0.13614,     0.13714,     0.13814,     0.13914,     0.14014,     0.14114,     0.14214,     0.14314,\n",
              "           0.14414,     0.14515,     0.14615,     0.14715,     0.14815,     0.14915,     0.15015,     0.15115,     0.15215,     0.15315,     0.15415,     0.15516,     0.15616,     0.15716,     0.15816,     0.15916,     0.16016,     0.16116,     0.16216,     0.16316,     0.16416,     0.16517,     0.16617,     0.16717,\n",
              "           0.16817,     0.16917,     0.17017,     0.17117,     0.17217,     0.17317,     0.17417,     0.17518,     0.17618,     0.17718,     0.17818,     0.17918,     0.18018,     0.18118,     0.18218,     0.18318,     0.18418,     0.18519,     0.18619,     0.18719,     0.18819,     0.18919,     0.19019,     0.19119,\n",
              "           0.19219,     0.19319,     0.19419,      0.1952,      0.1962,      0.1972,      0.1982,      0.1992,      0.2002,      0.2012,      0.2022,      0.2032,      0.2042,     0.20521,     0.20621,     0.20721,     0.20821,     0.20921,     0.21021,     0.21121,     0.21221,     0.21321,     0.21421,     0.21522,\n",
              "           0.21622,     0.21722,     0.21822,     0.21922,     0.22022,     0.22122,     0.22222,     0.22322,     0.22422,     0.22523,     0.22623,     0.22723,     0.22823,     0.22923,     0.23023,     0.23123,     0.23223,     0.23323,     0.23423,     0.23524,     0.23624,     0.23724,     0.23824,     0.23924,\n",
              "           0.24024,     0.24124,     0.24224,     0.24324,     0.24424,     0.24525,     0.24625,     0.24725,     0.24825,     0.24925,     0.25025,     0.25125,     0.25225,     0.25325,     0.25425,     0.25526,     0.25626,     0.25726,     0.25826,     0.25926,     0.26026,     0.26126,     0.26226,     0.26326,\n",
              "           0.26426,     0.26527,     0.26627,     0.26727,     0.26827,     0.26927,     0.27027,     0.27127,     0.27227,     0.27327,     0.27427,     0.27528,     0.27628,     0.27728,     0.27828,     0.27928,     0.28028,     0.28128,     0.28228,     0.28328,     0.28428,     0.28529,     0.28629,     0.28729,\n",
              "           0.28829,     0.28929,     0.29029,     0.29129,     0.29229,     0.29329,     0.29429,      0.2953,      0.2963,      0.2973,      0.2983,      0.2993,      0.3003,      0.3013,      0.3023,      0.3033,      0.3043,     0.30531,     0.30631,     0.30731,     0.30831,     0.30931,     0.31031,     0.31131,\n",
              "           0.31231,     0.31331,     0.31431,     0.31532,     0.31632,     0.31732,     0.31832,     0.31932,     0.32032,     0.32132,     0.32232,     0.32332,     0.32432,     0.32533,     0.32633,     0.32733,     0.32833,     0.32933,     0.33033,     0.33133,     0.33233,     0.33333,     0.33433,     0.33534,\n",
              "           0.33634,     0.33734,     0.33834,     0.33934,     0.34034,     0.34134,     0.34234,     0.34334,     0.34434,     0.34535,     0.34635,     0.34735,     0.34835,     0.34935,     0.35035,     0.35135,     0.35235,     0.35335,     0.35435,     0.35536,     0.35636,     0.35736,     0.35836,     0.35936,\n",
              "           0.36036,     0.36136,     0.36236,     0.36336,     0.36436,     0.36537,     0.36637,     0.36737,     0.36837,     0.36937,     0.37037,     0.37137,     0.37237,     0.37337,     0.37437,     0.37538,     0.37638,     0.37738,     0.37838,     0.37938,     0.38038,     0.38138,     0.38238,     0.38338,\n",
              "           0.38438,     0.38539,     0.38639,     0.38739,     0.38839,     0.38939,     0.39039,     0.39139,     0.39239,     0.39339,     0.39439,      0.3954,      0.3964,      0.3974,      0.3984,      0.3994,      0.4004,      0.4014,      0.4024,      0.4034,      0.4044,     0.40541,     0.40641,     0.40741,\n",
              "           0.40841,     0.40941,     0.41041,     0.41141,     0.41241,     0.41341,     0.41441,     0.41542,     0.41642,     0.41742,     0.41842,     0.41942,     0.42042,     0.42142,     0.42242,     0.42342,     0.42442,     0.42543,     0.42643,     0.42743,     0.42843,     0.42943,     0.43043,     0.43143,\n",
              "           0.43243,     0.43343,     0.43443,     0.43544,     0.43644,     0.43744,     0.43844,     0.43944,     0.44044,     0.44144,     0.44244,     0.44344,     0.44444,     0.44545,     0.44645,     0.44745,     0.44845,     0.44945,     0.45045,     0.45145,     0.45245,     0.45345,     0.45445,     0.45546,\n",
              "           0.45646,     0.45746,     0.45846,     0.45946,     0.46046,     0.46146,     0.46246,     0.46346,     0.46446,     0.46547,     0.46647,     0.46747,     0.46847,     0.46947,     0.47047,     0.47147,     0.47247,     0.47347,     0.47447,     0.47548,     0.47648,     0.47748,     0.47848,     0.47948,\n",
              "           0.48048,     0.48148,     0.48248,     0.48348,     0.48448,     0.48549,     0.48649,     0.48749,     0.48849,     0.48949,     0.49049,     0.49149,     0.49249,     0.49349,     0.49449,      0.4955,      0.4965,      0.4975,      0.4985,      0.4995,      0.5005,      0.5015,      0.5025,      0.5035,\n",
              "            0.5045,     0.50551,     0.50651,     0.50751,     0.50851,     0.50951,     0.51051,     0.51151,     0.51251,     0.51351,     0.51451,     0.51552,     0.51652,     0.51752,     0.51852,     0.51952,     0.52052,     0.52152,     0.52252,     0.52352,     0.52452,     0.52553,     0.52653,     0.52753,\n",
              "           0.52853,     0.52953,     0.53053,     0.53153,     0.53253,     0.53353,     0.53453,     0.53554,     0.53654,     0.53754,     0.53854,     0.53954,     0.54054,     0.54154,     0.54254,     0.54354,     0.54454,     0.54555,     0.54655,     0.54755,     0.54855,     0.54955,     0.55055,     0.55155,\n",
              "           0.55255,     0.55355,     0.55455,     0.55556,     0.55656,     0.55756,     0.55856,     0.55956,     0.56056,     0.56156,     0.56256,     0.56356,     0.56456,     0.56557,     0.56657,     0.56757,     0.56857,     0.56957,     0.57057,     0.57157,     0.57257,     0.57357,     0.57457,     0.57558,\n",
              "           0.57658,     0.57758,     0.57858,     0.57958,     0.58058,     0.58158,     0.58258,     0.58358,     0.58458,     0.58559,     0.58659,     0.58759,     0.58859,     0.58959,     0.59059,     0.59159,     0.59259,     0.59359,     0.59459,      0.5956,      0.5966,      0.5976,      0.5986,      0.5996,\n",
              "            0.6006,      0.6016,      0.6026,      0.6036,      0.6046,     0.60561,     0.60661,     0.60761,     0.60861,     0.60961,     0.61061,     0.61161,     0.61261,     0.61361,     0.61461,     0.61562,     0.61662,     0.61762,     0.61862,     0.61962,     0.62062,     0.62162,     0.62262,     0.62362,\n",
              "           0.62462,     0.62563,     0.62663,     0.62763,     0.62863,     0.62963,     0.63063,     0.63163,     0.63263,     0.63363,     0.63463,     0.63564,     0.63664,     0.63764,     0.63864,     0.63964,     0.64064,     0.64164,     0.64264,     0.64364,     0.64464,     0.64565,     0.64665,     0.64765,\n",
              "           0.64865,     0.64965,     0.65065,     0.65165,     0.65265,     0.65365,     0.65465,     0.65566,     0.65666,     0.65766,     0.65866,     0.65966,     0.66066,     0.66166,     0.66266,     0.66366,     0.66466,     0.66567,     0.66667,     0.66767,     0.66867,     0.66967,     0.67067,     0.67167,\n",
              "           0.67267,     0.67367,     0.67467,     0.67568,     0.67668,     0.67768,     0.67868,     0.67968,     0.68068,     0.68168,     0.68268,     0.68368,     0.68468,     0.68569,     0.68669,     0.68769,     0.68869,     0.68969,     0.69069,     0.69169,     0.69269,     0.69369,     0.69469,      0.6957,\n",
              "            0.6967,      0.6977,      0.6987,      0.6997,      0.7007,      0.7017,      0.7027,      0.7037,      0.7047,     0.70571,     0.70671,     0.70771,     0.70871,     0.70971,     0.71071,     0.71171,     0.71271,     0.71371,     0.71471,     0.71572,     0.71672,     0.71772,     0.71872,     0.71972,\n",
              "           0.72072,     0.72172,     0.72272,     0.72372,     0.72472,     0.72573,     0.72673,     0.72773,     0.72873,     0.72973,     0.73073,     0.73173,     0.73273,     0.73373,     0.73473,     0.73574,     0.73674,     0.73774,     0.73874,     0.73974,     0.74074,     0.74174,     0.74274,     0.74374,\n",
              "           0.74474,     0.74575,     0.74675,     0.74775,     0.74875,     0.74975,     0.75075,     0.75175,     0.75275,     0.75375,     0.75475,     0.75576,     0.75676,     0.75776,     0.75876,     0.75976,     0.76076,     0.76176,     0.76276,     0.76376,     0.76476,     0.76577,     0.76677,     0.76777,\n",
              "           0.76877,     0.76977,     0.77077,     0.77177,     0.77277,     0.77377,     0.77477,     0.77578,     0.77678,     0.77778,     0.77878,     0.77978,     0.78078,     0.78178,     0.78278,     0.78378,     0.78478,     0.78579,     0.78679,     0.78779,     0.78879,     0.78979,     0.79079,     0.79179,\n",
              "           0.79279,     0.79379,     0.79479,      0.7958,      0.7968,      0.7978,      0.7988,      0.7998,      0.8008,      0.8018,      0.8028,      0.8038,      0.8048,     0.80581,     0.80681,     0.80781,     0.80881,     0.80981,     0.81081,     0.81181,     0.81281,     0.81381,     0.81481,     0.81582,\n",
              "           0.81682,     0.81782,     0.81882,     0.81982,     0.82082,     0.82182,     0.82282,     0.82382,     0.82482,     0.82583,     0.82683,     0.82783,     0.82883,     0.82983,     0.83083,     0.83183,     0.83283,     0.83383,     0.83483,     0.83584,     0.83684,     0.83784,     0.83884,     0.83984,\n",
              "           0.84084,     0.84184,     0.84284,     0.84384,     0.84484,     0.84585,     0.84685,     0.84785,     0.84885,     0.84985,     0.85085,     0.85185,     0.85285,     0.85385,     0.85485,     0.85586,     0.85686,     0.85786,     0.85886,     0.85986,     0.86086,     0.86186,     0.86286,     0.86386,\n",
              "           0.86486,     0.86587,     0.86687,     0.86787,     0.86887,     0.86987,     0.87087,     0.87187,     0.87287,     0.87387,     0.87487,     0.87588,     0.87688,     0.87788,     0.87888,     0.87988,     0.88088,     0.88188,     0.88288,     0.88388,     0.88488,     0.88589,     0.88689,     0.88789,\n",
              "           0.88889,     0.88989,     0.89089,     0.89189,     0.89289,     0.89389,     0.89489,      0.8959,      0.8969,      0.8979,      0.8989,      0.8999,      0.9009,      0.9019,      0.9029,      0.9039,      0.9049,     0.90591,     0.90691,     0.90791,     0.90891,     0.90991,     0.91091,     0.91191,\n",
              "           0.91291,     0.91391,     0.91491,     0.91592,     0.91692,     0.91792,     0.91892,     0.91992,     0.92092,     0.92192,     0.92292,     0.92392,     0.92492,     0.92593,     0.92693,     0.92793,     0.92893,     0.92993,     0.93093,     0.93193,     0.93293,     0.93393,     0.93493,     0.93594,\n",
              "           0.93694,     0.93794,     0.93894,     0.93994,     0.94094,     0.94194,     0.94294,     0.94394,     0.94494,     0.94595,     0.94695,     0.94795,     0.94895,     0.94995,     0.95095,     0.95195,     0.95295,     0.95395,     0.95495,     0.95596,     0.95696,     0.95796,     0.95896,     0.95996,\n",
              "           0.96096,     0.96196,     0.96296,     0.96396,     0.96496,     0.96597,     0.96697,     0.96797,     0.96897,     0.96997,     0.97097,     0.97197,     0.97297,     0.97397,     0.97497,     0.97598,     0.97698,     0.97798,     0.97898,     0.97998,     0.98098,     0.98198,     0.98298,     0.98398,\n",
              "           0.98498,     0.98599,     0.98699,     0.98799,     0.98899,     0.98999,     0.99099,     0.99199,     0.99299,     0.99399,     0.99499,       0.996,       0.997,       0.998,       0.999,           1]), array([[    0.30102,     0.30102,     0.41762,     0.45866,     0.51419,     0.56158,     0.58174,     0.60612,     0.61291,     0.60872,     0.62762,     0.64225,     0.65315,     0.65932,     0.66755,     0.67726,     0.68388,     0.69031,     0.68806,     0.70142,      0.7014,     0.70394,     0.71721,\n",
              "            0.72425,     0.73567,     0.73702,     0.73796,     0.73889,     0.73982,     0.74074,     0.74295,     0.74854,     0.75344,      0.7553,     0.75709,     0.74995,     0.75536,     0.75412,       0.756,     0.75663,     0.75726,     0.75788,      0.7585,     0.75912,     0.75975,     0.76037,\n",
              "            0.76098,      0.7617,     0.76245,     0.76321,     0.76396,     0.76472,     0.76547,     0.76622,     0.76699,     0.76812,     0.76923,     0.77035,     0.77146,     0.77257,     0.76351,      0.7643,     0.76509,     0.76589,     0.76667,     0.76746,     0.76825,     0.76903,     0.76438,\n",
              "            0.75597,     0.75084,     0.75236,     0.75387,     0.75538,     0.75422,     0.75162,       0.749,     0.74638,     0.74484,     0.74346,     0.74208,      0.7407,     0.73931,     0.73793,     0.73654,      0.7375,     0.73994,     0.74181,     0.74108,     0.74035,     0.73962,     0.73889,\n",
              "            0.73816,     0.73743,      0.7367,     0.73596,     0.73523,      0.7345,     0.73376,     0.73303,     0.73229,     0.73182,     0.73237,     0.73291,     0.73346,       0.734,     0.73454,     0.73508,     0.73562,     0.73616,     0.73669,     0.73723,     0.73766,     0.73723,      0.7368,\n",
              "            0.73636,     0.73593,      0.7355,     0.73507,     0.73464,     0.73421,     0.73377,     0.73334,     0.73291,     0.73248,     0.73204,     0.73161,     0.73117,     0.73074,     0.73031,     0.72987,     0.72944,       0.729,     0.72857,     0.72813,      0.7277,     0.72729,     0.72797,\n",
              "            0.72865,     0.72932,     0.72999,     0.73067,     0.73134,       0.732,     0.73267,     0.73333,     0.73237,      0.7314,     0.73043,     0.72947,      0.7285,     0.72753,     0.72656,     0.72559,     0.72461,     0.72364,      0.7227,     0.72306,     0.72342,     0.72378,     0.72414,\n",
              "             0.7245,     0.72485,     0.72521,     0.72557,     0.72592,     0.72628,     0.72663,     0.72699,     0.72734,      0.7277,     0.72805,      0.7284,     0.72875,     0.72998,     0.73136,     0.73274,     0.73412,     0.73511,      0.7353,     0.73549,     0.73569,     0.73588,     0.73608,\n",
              "            0.73627,     0.73647,     0.73666,     0.73685,     0.73705,     0.73724,     0.73743,     0.73763,     0.73782,     0.73801,      0.7382,      0.7384,     0.73859,     0.73878,     0.73897,     0.73916,     0.73936,     0.73955,     0.73974,     0.73993,     0.74012,     0.74031,      0.7405,\n",
              "            0.74069,     0.74088,     0.74107,     0.74126,     0.74168,     0.74244,     0.74321,     0.74396,     0.74472,     0.74547,     0.74623,     0.74697,     0.74772,     0.74791,       0.748,     0.74809,     0.74818,     0.74827,     0.74836,     0.74846,     0.74855,     0.74864,     0.74873,\n",
              "            0.74882,     0.74891,     0.74901,      0.7491,     0.74919,     0.74928,     0.74937,     0.74946,     0.74955,     0.74965,     0.74974,     0.74983,     0.74992,     0.75001,      0.7501,     0.75019,     0.75028,     0.75037,     0.75047,     0.75056,     0.75065,     0.75074,     0.75083,\n",
              "            0.75092,     0.75101,      0.7511,     0.75119,     0.75128,     0.75137,     0.75146,     0.75155,     0.75165,     0.75174,     0.75183,     0.75192,     0.75201,      0.7521,     0.75219,     0.75228,     0.75237,     0.75246,     0.75255,     0.75264,     0.75273,     0.75282,     0.75291,\n",
              "              0.753,     0.75309,     0.75318,     0.75327,     0.75336,     0.75345,     0.75354,     0.75363,     0.75372,     0.75381,      0.7539,     0.75399,     0.75408,     0.75417,     0.75426,     0.75435,     0.75444,     0.75454,     0.75464,     0.75474,     0.75484,     0.75494,     0.75504,\n",
              "            0.75514,     0.75524,     0.75534,     0.75544,     0.75554,     0.75564,     0.75574,     0.75584,     0.75594,     0.75604,     0.75614,     0.75624,     0.75634,     0.75644,     0.75654,     0.75663,     0.75673,     0.75683,     0.75693,     0.75703,     0.75713,     0.75723,     0.75733,\n",
              "            0.75743,     0.75753,     0.75763,     0.75772,     0.75782,     0.75792,     0.75802,     0.75812,     0.75822,     0.75832,     0.75842,     0.75851,     0.75861,     0.75871,     0.75881,     0.75891,     0.75901,      0.7591,      0.7592,      0.7593,      0.7594,      0.7595,     0.75959,\n",
              "            0.75969,     0.75979,     0.75989,     0.75999,     0.76008,     0.76018,     0.76028,     0.76038,     0.76048,     0.76057,     0.76067,     0.76077,     0.76087,     0.76096,     0.76106,     0.76093,     0.76079,     0.76065,     0.76052,     0.76038,     0.76024,     0.76011,     0.75997,\n",
              "            0.75983,      0.7597,     0.75956,     0.75942,     0.75929,     0.75915,     0.75901,     0.75887,     0.75874,      0.7586,     0.75846,     0.75833,     0.75819,     0.75805,     0.75791,     0.75778,     0.75764,      0.7575,     0.75736,     0.75723,     0.75709,     0.75695,     0.75681,\n",
              "            0.75668,     0.75654,      0.7564,     0.75626,     0.75613,     0.75599,     0.75585,     0.75571,     0.75557,     0.75544,      0.7553,     0.75516,     0.75502,     0.75488,     0.75475,     0.75461,     0.75447,     0.75433,     0.75419,     0.75406,     0.75392,     0.75378,     0.75364,\n",
              "             0.7535,     0.75336,     0.75323,     0.75309,     0.75295,     0.75281,     0.75267,     0.75253,      0.7524,     0.75226,     0.75212,     0.75198,     0.75184,      0.7517,     0.75156,     0.75143,     0.75129,     0.75115,     0.75101,     0.75087,     0.75073,     0.75059,     0.75045,\n",
              "            0.75031,     0.75017,     0.75004,     0.74145,     0.73851,     0.73822,     0.73793,     0.73763,     0.73734,     0.73705,     0.73675,     0.73646,     0.73616,     0.73587,     0.73558,     0.73528,     0.73499,     0.73469,      0.7344,      0.7341,     0.73381,     0.73351,     0.73322,\n",
              "            0.73292,     0.73262,     0.73233,     0.73203,     0.73174,     0.73144,     0.73114,     0.73085,     0.73055,     0.73025,     0.72996,     0.72966,     0.72936,     0.72906,     0.72877,     0.72847,     0.72817,     0.72787,     0.72757,     0.72728,     0.72717,     0.72707,     0.72696,\n",
              "            0.72686,     0.72676,     0.72665,     0.72655,     0.72645,     0.72635,     0.72624,     0.72614,     0.72604,     0.72593,     0.72583,     0.72573,     0.72562,     0.72552,     0.72542,     0.72531,     0.72521,      0.7251,       0.725,      0.7249,     0.72479,     0.72469,     0.72459,\n",
              "            0.72448,     0.72438,     0.72428,     0.72417,     0.72407,     0.72397,     0.72386,     0.72376,     0.72365,     0.72355,     0.72345,     0.72334,     0.72324,     0.72314,     0.72303,     0.72293,     0.72282,     0.72272,     0.72262,     0.72251,     0.72241,      0.7223,      0.7222,\n",
              "             0.7221,     0.72199,     0.72189,     0.72178,     0.72168,     0.72158,     0.72147,     0.72137,     0.72126,     0.72116,     0.72106,     0.72095,     0.72085,     0.72074,     0.72064,     0.72054,     0.72043,     0.72033,     0.72022,     0.72012,     0.72001,     0.71991,     0.71981,\n",
              "             0.7197,      0.7196,     0.71949,     0.71939,     0.71928,     0.71918,     0.71907,     0.71897,     0.71887,     0.71876,     0.71866,     0.71855,     0.71845,     0.71834,     0.71824,     0.71813,     0.71803,     0.71792,     0.71782,     0.71771,     0.71761,     0.71751,      0.7174,\n",
              "             0.7173,     0.71719,     0.71709,     0.71698,     0.71688,     0.71677,     0.71667,     0.71656,     0.71646,     0.71635,     0.71625,     0.71614,     0.71604,     0.71593,     0.71583,     0.71572,     0.71562,     0.71078,     0.70466,     0.70073,     0.69719,     0.69363,     0.69132,\n",
              "            0.69069,     0.69006,     0.68944,     0.68881,     0.68818,     0.68755,     0.68692,     0.68628,     0.68565,     0.68502,     0.68439,     0.68375,     0.68312,     0.68248,     0.68185,     0.68121,     0.68057,     0.67994,      0.6793,     0.67757,     0.67574,      0.6739,     0.67206,\n",
              "            0.67021,     0.66836,     0.66663,     0.66622,     0.66582,     0.66541,       0.665,     0.66459,     0.66419,     0.66378,     0.66337,     0.66296,     0.66255,     0.66214,     0.66173,     0.66132,     0.66091,      0.6605,     0.66009,     0.65968,     0.65927,     0.65886,     0.65845,\n",
              "            0.65803,     0.65762,     0.65721,      0.6568,     0.65638,     0.65597,     0.65556,     0.65514,     0.65473,     0.65431,      0.6539,     0.65292,     0.65186,     0.65079,     0.64973,     0.64866,     0.64759,     0.64651,     0.64544,     0.64437,     0.64329,     0.64221,     0.64113,\n",
              "             0.6362,     0.62935,     0.62459,     0.62062,     0.61663,     0.61262,     0.60858,     0.60452,     0.60043,     0.59632,     0.59219,     0.58803,     0.58116,     0.57136,     0.56708,     0.56277,     0.55843,      0.5521,     0.54438,     0.53877,     0.53428,     0.52978,     0.52556,\n",
              "            0.52237,     0.51917,     0.51595,     0.51272,     0.50897,     0.50431,     0.49961,     0.49489,     0.48939,      0.4838,     0.47812,     0.46959,     0.46078,     0.44912,     0.40685,     0.40313,     0.39939,     0.39563,     0.39185,     0.38687,     0.38141,     0.37591,     0.32912,\n",
              "            0.31538,     0.28664,     0.27944,     0.27219,     0.26149,     0.25035,      0.2354,     0.22397,     0.21623,     0.20844,     0.20303,     0.19938,     0.19572,     0.19205,     0.18836,     0.18465,     0.17798,     0.16163,     0.15146,     0.14309,     0.13463,     0.12246,     0.10947,\n",
              "            0.10587,     0.10336,     0.10085,    0.098334,    0.095809,    0.093278,    0.090739,    0.088194,    0.085642,    0.083083,     0.08089,     0.07889,    0.076886,    0.074879,    0.072867,     0.07085,     0.06883,    0.066805,    0.064776,    0.062743,    0.060705,    0.058663,    0.056617,\n",
              "           0.054635,    0.052722,    0.050806,    0.048885,    0.046961,    0.045033,    0.043102,    0.041166,    0.039227,    0.037283,    0.035336,    0.033385,    0.031431,    0.029472,           0,           0,           0,           0,           0,           0,           0,           0,           0,\n",
              "                  0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,\n",
              "                  0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,\n",
              "                  0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,\n",
              "                  0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,\n",
              "                  0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,\n",
              "                  0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,\n",
              "                  0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,\n",
              "                  0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,\n",
              "                  0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,\n",
              "                  0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0]]), 'Confidence', 'F1'], [array([          0,    0.001001,    0.002002,    0.003003,    0.004004,    0.005005,    0.006006,    0.007007,    0.008008,    0.009009,     0.01001,    0.011011,    0.012012,    0.013013,    0.014014,    0.015015,    0.016016,    0.017017,    0.018018,    0.019019,     0.02002,    0.021021,    0.022022,    0.023023,\n",
              "          0.024024,    0.025025,    0.026026,    0.027027,    0.028028,    0.029029,     0.03003,    0.031031,    0.032032,    0.033033,    0.034034,    0.035035,    0.036036,    0.037037,    0.038038,    0.039039,     0.04004,    0.041041,    0.042042,    0.043043,    0.044044,    0.045045,    0.046046,    0.047047,\n",
              "          0.048048,    0.049049,     0.05005,    0.051051,    0.052052,    0.053053,    0.054054,    0.055055,    0.056056,    0.057057,    0.058058,    0.059059,     0.06006,    0.061061,    0.062062,    0.063063,    0.064064,    0.065065,    0.066066,    0.067067,    0.068068,    0.069069,     0.07007,    0.071071,\n",
              "          0.072072,    0.073073,    0.074074,    0.075075,    0.076076,    0.077077,    0.078078,    0.079079,     0.08008,    0.081081,    0.082082,    0.083083,    0.084084,    0.085085,    0.086086,    0.087087,    0.088088,    0.089089,     0.09009,    0.091091,    0.092092,    0.093093,    0.094094,    0.095095,\n",
              "          0.096096,    0.097097,    0.098098,    0.099099,      0.1001,      0.1011,      0.1021,      0.1031,      0.1041,     0.10511,     0.10611,     0.10711,     0.10811,     0.10911,     0.11011,     0.11111,     0.11211,     0.11311,     0.11411,     0.11512,     0.11612,     0.11712,     0.11812,     0.11912,\n",
              "           0.12012,     0.12112,     0.12212,     0.12312,     0.12412,     0.12513,     0.12613,     0.12713,     0.12813,     0.12913,     0.13013,     0.13113,     0.13213,     0.13313,     0.13413,     0.13514,     0.13614,     0.13714,     0.13814,     0.13914,     0.14014,     0.14114,     0.14214,     0.14314,\n",
              "           0.14414,     0.14515,     0.14615,     0.14715,     0.14815,     0.14915,     0.15015,     0.15115,     0.15215,     0.15315,     0.15415,     0.15516,     0.15616,     0.15716,     0.15816,     0.15916,     0.16016,     0.16116,     0.16216,     0.16316,     0.16416,     0.16517,     0.16617,     0.16717,\n",
              "           0.16817,     0.16917,     0.17017,     0.17117,     0.17217,     0.17317,     0.17417,     0.17518,     0.17618,     0.17718,     0.17818,     0.17918,     0.18018,     0.18118,     0.18218,     0.18318,     0.18418,     0.18519,     0.18619,     0.18719,     0.18819,     0.18919,     0.19019,     0.19119,\n",
              "           0.19219,     0.19319,     0.19419,      0.1952,      0.1962,      0.1972,      0.1982,      0.1992,      0.2002,      0.2012,      0.2022,      0.2032,      0.2042,     0.20521,     0.20621,     0.20721,     0.20821,     0.20921,     0.21021,     0.21121,     0.21221,     0.21321,     0.21421,     0.21522,\n",
              "           0.21622,     0.21722,     0.21822,     0.21922,     0.22022,     0.22122,     0.22222,     0.22322,     0.22422,     0.22523,     0.22623,     0.22723,     0.22823,     0.22923,     0.23023,     0.23123,     0.23223,     0.23323,     0.23423,     0.23524,     0.23624,     0.23724,     0.23824,     0.23924,\n",
              "           0.24024,     0.24124,     0.24224,     0.24324,     0.24424,     0.24525,     0.24625,     0.24725,     0.24825,     0.24925,     0.25025,     0.25125,     0.25225,     0.25325,     0.25425,     0.25526,     0.25626,     0.25726,     0.25826,     0.25926,     0.26026,     0.26126,     0.26226,     0.26326,\n",
              "           0.26426,     0.26527,     0.26627,     0.26727,     0.26827,     0.26927,     0.27027,     0.27127,     0.27227,     0.27327,     0.27427,     0.27528,     0.27628,     0.27728,     0.27828,     0.27928,     0.28028,     0.28128,     0.28228,     0.28328,     0.28428,     0.28529,     0.28629,     0.28729,\n",
              "           0.28829,     0.28929,     0.29029,     0.29129,     0.29229,     0.29329,     0.29429,      0.2953,      0.2963,      0.2973,      0.2983,      0.2993,      0.3003,      0.3013,      0.3023,      0.3033,      0.3043,     0.30531,     0.30631,     0.30731,     0.30831,     0.30931,     0.31031,     0.31131,\n",
              "           0.31231,     0.31331,     0.31431,     0.31532,     0.31632,     0.31732,     0.31832,     0.31932,     0.32032,     0.32132,     0.32232,     0.32332,     0.32432,     0.32533,     0.32633,     0.32733,     0.32833,     0.32933,     0.33033,     0.33133,     0.33233,     0.33333,     0.33433,     0.33534,\n",
              "           0.33634,     0.33734,     0.33834,     0.33934,     0.34034,     0.34134,     0.34234,     0.34334,     0.34434,     0.34535,     0.34635,     0.34735,     0.34835,     0.34935,     0.35035,     0.35135,     0.35235,     0.35335,     0.35435,     0.35536,     0.35636,     0.35736,     0.35836,     0.35936,\n",
              "           0.36036,     0.36136,     0.36236,     0.36336,     0.36436,     0.36537,     0.36637,     0.36737,     0.36837,     0.36937,     0.37037,     0.37137,     0.37237,     0.37337,     0.37437,     0.37538,     0.37638,     0.37738,     0.37838,     0.37938,     0.38038,     0.38138,     0.38238,     0.38338,\n",
              "           0.38438,     0.38539,     0.38639,     0.38739,     0.38839,     0.38939,     0.39039,     0.39139,     0.39239,     0.39339,     0.39439,      0.3954,      0.3964,      0.3974,      0.3984,      0.3994,      0.4004,      0.4014,      0.4024,      0.4034,      0.4044,     0.40541,     0.40641,     0.40741,\n",
              "           0.40841,     0.40941,     0.41041,     0.41141,     0.41241,     0.41341,     0.41441,     0.41542,     0.41642,     0.41742,     0.41842,     0.41942,     0.42042,     0.42142,     0.42242,     0.42342,     0.42442,     0.42543,     0.42643,     0.42743,     0.42843,     0.42943,     0.43043,     0.43143,\n",
              "           0.43243,     0.43343,     0.43443,     0.43544,     0.43644,     0.43744,     0.43844,     0.43944,     0.44044,     0.44144,     0.44244,     0.44344,     0.44444,     0.44545,     0.44645,     0.44745,     0.44845,     0.44945,     0.45045,     0.45145,     0.45245,     0.45345,     0.45445,     0.45546,\n",
              "           0.45646,     0.45746,     0.45846,     0.45946,     0.46046,     0.46146,     0.46246,     0.46346,     0.46446,     0.46547,     0.46647,     0.46747,     0.46847,     0.46947,     0.47047,     0.47147,     0.47247,     0.47347,     0.47447,     0.47548,     0.47648,     0.47748,     0.47848,     0.47948,\n",
              "           0.48048,     0.48148,     0.48248,     0.48348,     0.48448,     0.48549,     0.48649,     0.48749,     0.48849,     0.48949,     0.49049,     0.49149,     0.49249,     0.49349,     0.49449,      0.4955,      0.4965,      0.4975,      0.4985,      0.4995,      0.5005,      0.5015,      0.5025,      0.5035,\n",
              "            0.5045,     0.50551,     0.50651,     0.50751,     0.50851,     0.50951,     0.51051,     0.51151,     0.51251,     0.51351,     0.51451,     0.51552,     0.51652,     0.51752,     0.51852,     0.51952,     0.52052,     0.52152,     0.52252,     0.52352,     0.52452,     0.52553,     0.52653,     0.52753,\n",
              "           0.52853,     0.52953,     0.53053,     0.53153,     0.53253,     0.53353,     0.53453,     0.53554,     0.53654,     0.53754,     0.53854,     0.53954,     0.54054,     0.54154,     0.54254,     0.54354,     0.54454,     0.54555,     0.54655,     0.54755,     0.54855,     0.54955,     0.55055,     0.55155,\n",
              "           0.55255,     0.55355,     0.55455,     0.55556,     0.55656,     0.55756,     0.55856,     0.55956,     0.56056,     0.56156,     0.56256,     0.56356,     0.56456,     0.56557,     0.56657,     0.56757,     0.56857,     0.56957,     0.57057,     0.57157,     0.57257,     0.57357,     0.57457,     0.57558,\n",
              "           0.57658,     0.57758,     0.57858,     0.57958,     0.58058,     0.58158,     0.58258,     0.58358,     0.58458,     0.58559,     0.58659,     0.58759,     0.58859,     0.58959,     0.59059,     0.59159,     0.59259,     0.59359,     0.59459,      0.5956,      0.5966,      0.5976,      0.5986,      0.5996,\n",
              "            0.6006,      0.6016,      0.6026,      0.6036,      0.6046,     0.60561,     0.60661,     0.60761,     0.60861,     0.60961,     0.61061,     0.61161,     0.61261,     0.61361,     0.61461,     0.61562,     0.61662,     0.61762,     0.61862,     0.61962,     0.62062,     0.62162,     0.62262,     0.62362,\n",
              "           0.62462,     0.62563,     0.62663,     0.62763,     0.62863,     0.62963,     0.63063,     0.63163,     0.63263,     0.63363,     0.63463,     0.63564,     0.63664,     0.63764,     0.63864,     0.63964,     0.64064,     0.64164,     0.64264,     0.64364,     0.64464,     0.64565,     0.64665,     0.64765,\n",
              "           0.64865,     0.64965,     0.65065,     0.65165,     0.65265,     0.65365,     0.65465,     0.65566,     0.65666,     0.65766,     0.65866,     0.65966,     0.66066,     0.66166,     0.66266,     0.66366,     0.66466,     0.66567,     0.66667,     0.66767,     0.66867,     0.66967,     0.67067,     0.67167,\n",
              "           0.67267,     0.67367,     0.67467,     0.67568,     0.67668,     0.67768,     0.67868,     0.67968,     0.68068,     0.68168,     0.68268,     0.68368,     0.68468,     0.68569,     0.68669,     0.68769,     0.68869,     0.68969,     0.69069,     0.69169,     0.69269,     0.69369,     0.69469,      0.6957,\n",
              "            0.6967,      0.6977,      0.6987,      0.6997,      0.7007,      0.7017,      0.7027,      0.7037,      0.7047,     0.70571,     0.70671,     0.70771,     0.70871,     0.70971,     0.71071,     0.71171,     0.71271,     0.71371,     0.71471,     0.71572,     0.71672,     0.71772,     0.71872,     0.71972,\n",
              "           0.72072,     0.72172,     0.72272,     0.72372,     0.72472,     0.72573,     0.72673,     0.72773,     0.72873,     0.72973,     0.73073,     0.73173,     0.73273,     0.73373,     0.73473,     0.73574,     0.73674,     0.73774,     0.73874,     0.73974,     0.74074,     0.74174,     0.74274,     0.74374,\n",
              "           0.74474,     0.74575,     0.74675,     0.74775,     0.74875,     0.74975,     0.75075,     0.75175,     0.75275,     0.75375,     0.75475,     0.75576,     0.75676,     0.75776,     0.75876,     0.75976,     0.76076,     0.76176,     0.76276,     0.76376,     0.76476,     0.76577,     0.76677,     0.76777,\n",
              "           0.76877,     0.76977,     0.77077,     0.77177,     0.77277,     0.77377,     0.77477,     0.77578,     0.77678,     0.77778,     0.77878,     0.77978,     0.78078,     0.78178,     0.78278,     0.78378,     0.78478,     0.78579,     0.78679,     0.78779,     0.78879,     0.78979,     0.79079,     0.79179,\n",
              "           0.79279,     0.79379,     0.79479,      0.7958,      0.7968,      0.7978,      0.7988,      0.7998,      0.8008,      0.8018,      0.8028,      0.8038,      0.8048,     0.80581,     0.80681,     0.80781,     0.80881,     0.80981,     0.81081,     0.81181,     0.81281,     0.81381,     0.81481,     0.81582,\n",
              "           0.81682,     0.81782,     0.81882,     0.81982,     0.82082,     0.82182,     0.82282,     0.82382,     0.82482,     0.82583,     0.82683,     0.82783,     0.82883,     0.82983,     0.83083,     0.83183,     0.83283,     0.83383,     0.83483,     0.83584,     0.83684,     0.83784,     0.83884,     0.83984,\n",
              "           0.84084,     0.84184,     0.84284,     0.84384,     0.84484,     0.84585,     0.84685,     0.84785,     0.84885,     0.84985,     0.85085,     0.85185,     0.85285,     0.85385,     0.85485,     0.85586,     0.85686,     0.85786,     0.85886,     0.85986,     0.86086,     0.86186,     0.86286,     0.86386,\n",
              "           0.86486,     0.86587,     0.86687,     0.86787,     0.86887,     0.86987,     0.87087,     0.87187,     0.87287,     0.87387,     0.87487,     0.87588,     0.87688,     0.87788,     0.87888,     0.87988,     0.88088,     0.88188,     0.88288,     0.88388,     0.88488,     0.88589,     0.88689,     0.88789,\n",
              "           0.88889,     0.88989,     0.89089,     0.89189,     0.89289,     0.89389,     0.89489,      0.8959,      0.8969,      0.8979,      0.8989,      0.8999,      0.9009,      0.9019,      0.9029,      0.9039,      0.9049,     0.90591,     0.90691,     0.90791,     0.90891,     0.90991,     0.91091,     0.91191,\n",
              "           0.91291,     0.91391,     0.91491,     0.91592,     0.91692,     0.91792,     0.91892,     0.91992,     0.92092,     0.92192,     0.92292,     0.92392,     0.92492,     0.92593,     0.92693,     0.92793,     0.92893,     0.92993,     0.93093,     0.93193,     0.93293,     0.93393,     0.93493,     0.93594,\n",
              "           0.93694,     0.93794,     0.93894,     0.93994,     0.94094,     0.94194,     0.94294,     0.94394,     0.94494,     0.94595,     0.94695,     0.94795,     0.94895,     0.94995,     0.95095,     0.95195,     0.95295,     0.95395,     0.95495,     0.95596,     0.95696,     0.95796,     0.95896,     0.95996,\n",
              "           0.96096,     0.96196,     0.96296,     0.96396,     0.96496,     0.96597,     0.96697,     0.96797,     0.96897,     0.96997,     0.97097,     0.97197,     0.97297,     0.97397,     0.97497,     0.97598,     0.97698,     0.97798,     0.97898,     0.97998,     0.98098,     0.98198,     0.98298,     0.98398,\n",
              "           0.98498,     0.98599,     0.98699,     0.98799,     0.98899,     0.98999,     0.99099,     0.99199,     0.99299,     0.99399,     0.99499,       0.996,       0.997,       0.998,       0.999,           1]), array([[    0.18323,     0.18323,     0.27758,     0.31709,     0.37276,     0.42473,     0.45251,     0.48272,     0.49674,     0.49681,     0.52249,     0.54308,     0.55886,     0.56795,     0.58028,     0.59512,     0.60542,     0.61718,     0.62095,     0.64305,     0.64836,     0.65772,     0.68128,\n",
              "             0.6941,     0.71539,     0.71795,     0.71972,     0.72149,     0.72327,     0.72504,     0.72928,     0.74013,     0.74977,     0.75347,     0.75712,     0.75431,     0.76829,     0.78152,     0.78558,     0.78694,     0.78829,     0.78964,       0.791,     0.79235,     0.79371,     0.79506,\n",
              "            0.79641,     0.79797,     0.79964,     0.80131,     0.80297,     0.80464,     0.80631,     0.80797,      0.8097,      0.8122,      0.8147,     0.81721,     0.81971,     0.82222,     0.82001,     0.82185,     0.82368,     0.82552,     0.82736,     0.82919,     0.83103,     0.83287,     0.83189,\n",
              "            0.82938,     0.82963,     0.83335,     0.83708,      0.8408,     0.84162,     0.84088,     0.84013,     0.83938,     0.83894,     0.83853,     0.83813,     0.83773,     0.83733,     0.83692,     0.83652,     0.84025,      0.8466,     0.85182,     0.85162,     0.85142,     0.85122,     0.85102,\n",
              "            0.85082,     0.85062,     0.85042,     0.85022,     0.85002,     0.84982,     0.84962,     0.84941,     0.84921,     0.84937,     0.85084,     0.85231,     0.85378,     0.85525,     0.85672,     0.85819,     0.85966,     0.86114,     0.86261,     0.86408,     0.86537,     0.86526,     0.86515,\n",
              "            0.86504,     0.86493,     0.86482,     0.86471,     0.86461,      0.8645,     0.86439,     0.86428,     0.86417,     0.86406,     0.86395,     0.86384,     0.86373,     0.86362,     0.86351,      0.8634,     0.86329,     0.86318,     0.86307,     0.86296,     0.86285,      0.8628,     0.86471,\n",
              "            0.86662,     0.86853,     0.87045,     0.87236,     0.87427,     0.87618,     0.87809,        0.88,     0.87978,     0.87955,     0.87933,     0.87911,     0.87888,     0.87866,     0.87844,     0.87821,     0.87799,     0.87777,     0.87758,     0.87864,     0.87971,     0.88077,     0.88183,\n",
              "             0.8829,     0.88396,     0.88502,     0.88609,     0.88715,     0.88821,     0.88928,     0.89034,      0.8914,     0.89247,     0.89353,     0.89459,     0.89566,     0.89935,     0.90358,      0.9078,     0.91203,     0.91509,     0.91569,      0.9163,      0.9169,      0.9175,     0.91811,\n",
              "            0.91871,     0.91932,     0.91992,     0.92052,     0.92113,     0.92173,     0.92234,     0.92294,     0.92354,     0.92415,     0.92475,     0.92536,     0.92596,     0.92656,     0.92717,     0.92777,     0.92838,     0.92898,     0.92958,     0.93019,     0.93079,      0.9314,       0.932,\n",
              "            0.93261,     0.93321,     0.93381,     0.93442,     0.93574,     0.93818,     0.94061,     0.94305,     0.94548,     0.94791,     0.95035,     0.95278,     0.95521,     0.95581,     0.95611,     0.95641,     0.95671,     0.95701,     0.95732,     0.95762,     0.95792,     0.95822,     0.95852,\n",
              "            0.95882,     0.95912,     0.95942,     0.95972,     0.96002,     0.96032,     0.96062,     0.96092,     0.96122,     0.96152,     0.96182,     0.96212,     0.96242,     0.96272,     0.96302,     0.96332,     0.96362,     0.96392,     0.96422,     0.96452,     0.96482,     0.96512,     0.96542,\n",
              "            0.96572,     0.96602,     0.96632,     0.96662,     0.96693,     0.96723,     0.96753,     0.96783,     0.96813,     0.96843,     0.96873,     0.96903,     0.96933,     0.96963,     0.96993,     0.97023,     0.97053,     0.97083,     0.97113,     0.97143,     0.97173,     0.97203,     0.97233,\n",
              "            0.97263,     0.97293,     0.97323,     0.97353,     0.97383,     0.97413,     0.97443,     0.97473,     0.97503,     0.97533,     0.97563,     0.97593,     0.97623,     0.97654,     0.97684,     0.97714,     0.97746,     0.97779,     0.97813,     0.97846,      0.9788,     0.97914,     0.97947,\n",
              "            0.97981,     0.98015,     0.98048,     0.98082,     0.98116,     0.98149,     0.98183,     0.98217,      0.9825,     0.98284,     0.98317,     0.98351,     0.98385,     0.98418,     0.98452,     0.98486,     0.98519,     0.98553,     0.98587,      0.9862,     0.98654,     0.98687,     0.98721,\n",
              "            0.98755,     0.98788,     0.98822,     0.98856,     0.98889,     0.98923,     0.98957,      0.9899,     0.99024,     0.99058,     0.99091,     0.99125,     0.99158,     0.99192,     0.99226,     0.99259,     0.99293,     0.99327,      0.9936,     0.99394,     0.99428,     0.99461,     0.99495,\n",
              "            0.99529,     0.99562,     0.99596,     0.99629,     0.99663,     0.99697,      0.9973,     0.99764,     0.99798,     0.99831,     0.99865,     0.99899,     0.99932,     0.99966,     0.99999,           1,           1,           1,           1,           1,           1,           1,           1,\n",
              "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
              "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
              "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
              "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
              "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
              "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
              "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
              "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
              "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
              "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
              "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
              "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
              "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
              "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
              "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
              "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
              "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
              "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
              "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
              "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
              "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
              "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
              "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
              "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
              "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
              "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
              "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
              "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1]]), 'Confidence', 'Precision'], [array([          0,    0.001001,    0.002002,    0.003003,    0.004004,    0.005005,    0.006006,    0.007007,    0.008008,    0.009009,     0.01001,    0.011011,    0.012012,    0.013013,    0.014014,    0.015015,    0.016016,    0.017017,    0.018018,    0.019019,     0.02002,    0.021021,    0.022022,    0.023023,\n",
              "          0.024024,    0.025025,    0.026026,    0.027027,    0.028028,    0.029029,     0.03003,    0.031031,    0.032032,    0.033033,    0.034034,    0.035035,    0.036036,    0.037037,    0.038038,    0.039039,     0.04004,    0.041041,    0.042042,    0.043043,    0.044044,    0.045045,    0.046046,    0.047047,\n",
              "          0.048048,    0.049049,     0.05005,    0.051051,    0.052052,    0.053053,    0.054054,    0.055055,    0.056056,    0.057057,    0.058058,    0.059059,     0.06006,    0.061061,    0.062062,    0.063063,    0.064064,    0.065065,    0.066066,    0.067067,    0.068068,    0.069069,     0.07007,    0.071071,\n",
              "          0.072072,    0.073073,    0.074074,    0.075075,    0.076076,    0.077077,    0.078078,    0.079079,     0.08008,    0.081081,    0.082082,    0.083083,    0.084084,    0.085085,    0.086086,    0.087087,    0.088088,    0.089089,     0.09009,    0.091091,    0.092092,    0.093093,    0.094094,    0.095095,\n",
              "          0.096096,    0.097097,    0.098098,    0.099099,      0.1001,      0.1011,      0.1021,      0.1031,      0.1041,     0.10511,     0.10611,     0.10711,     0.10811,     0.10911,     0.11011,     0.11111,     0.11211,     0.11311,     0.11411,     0.11512,     0.11612,     0.11712,     0.11812,     0.11912,\n",
              "           0.12012,     0.12112,     0.12212,     0.12312,     0.12412,     0.12513,     0.12613,     0.12713,     0.12813,     0.12913,     0.13013,     0.13113,     0.13213,     0.13313,     0.13413,     0.13514,     0.13614,     0.13714,     0.13814,     0.13914,     0.14014,     0.14114,     0.14214,     0.14314,\n",
              "           0.14414,     0.14515,     0.14615,     0.14715,     0.14815,     0.14915,     0.15015,     0.15115,     0.15215,     0.15315,     0.15415,     0.15516,     0.15616,     0.15716,     0.15816,     0.15916,     0.16016,     0.16116,     0.16216,     0.16316,     0.16416,     0.16517,     0.16617,     0.16717,\n",
              "           0.16817,     0.16917,     0.17017,     0.17117,     0.17217,     0.17317,     0.17417,     0.17518,     0.17618,     0.17718,     0.17818,     0.17918,     0.18018,     0.18118,     0.18218,     0.18318,     0.18418,     0.18519,     0.18619,     0.18719,     0.18819,     0.18919,     0.19019,     0.19119,\n",
              "           0.19219,     0.19319,     0.19419,      0.1952,      0.1962,      0.1972,      0.1982,      0.1992,      0.2002,      0.2012,      0.2022,      0.2032,      0.2042,     0.20521,     0.20621,     0.20721,     0.20821,     0.20921,     0.21021,     0.21121,     0.21221,     0.21321,     0.21421,     0.21522,\n",
              "           0.21622,     0.21722,     0.21822,     0.21922,     0.22022,     0.22122,     0.22222,     0.22322,     0.22422,     0.22523,     0.22623,     0.22723,     0.22823,     0.22923,     0.23023,     0.23123,     0.23223,     0.23323,     0.23423,     0.23524,     0.23624,     0.23724,     0.23824,     0.23924,\n",
              "           0.24024,     0.24124,     0.24224,     0.24324,     0.24424,     0.24525,     0.24625,     0.24725,     0.24825,     0.24925,     0.25025,     0.25125,     0.25225,     0.25325,     0.25425,     0.25526,     0.25626,     0.25726,     0.25826,     0.25926,     0.26026,     0.26126,     0.26226,     0.26326,\n",
              "           0.26426,     0.26527,     0.26627,     0.26727,     0.26827,     0.26927,     0.27027,     0.27127,     0.27227,     0.27327,     0.27427,     0.27528,     0.27628,     0.27728,     0.27828,     0.27928,     0.28028,     0.28128,     0.28228,     0.28328,     0.28428,     0.28529,     0.28629,     0.28729,\n",
              "           0.28829,     0.28929,     0.29029,     0.29129,     0.29229,     0.29329,     0.29429,      0.2953,      0.2963,      0.2973,      0.2983,      0.2993,      0.3003,      0.3013,      0.3023,      0.3033,      0.3043,     0.30531,     0.30631,     0.30731,     0.30831,     0.30931,     0.31031,     0.31131,\n",
              "           0.31231,     0.31331,     0.31431,     0.31532,     0.31632,     0.31732,     0.31832,     0.31932,     0.32032,     0.32132,     0.32232,     0.32332,     0.32432,     0.32533,     0.32633,     0.32733,     0.32833,     0.32933,     0.33033,     0.33133,     0.33233,     0.33333,     0.33433,     0.33534,\n",
              "           0.33634,     0.33734,     0.33834,     0.33934,     0.34034,     0.34134,     0.34234,     0.34334,     0.34434,     0.34535,     0.34635,     0.34735,     0.34835,     0.34935,     0.35035,     0.35135,     0.35235,     0.35335,     0.35435,     0.35536,     0.35636,     0.35736,     0.35836,     0.35936,\n",
              "           0.36036,     0.36136,     0.36236,     0.36336,     0.36436,     0.36537,     0.36637,     0.36737,     0.36837,     0.36937,     0.37037,     0.37137,     0.37237,     0.37337,     0.37437,     0.37538,     0.37638,     0.37738,     0.37838,     0.37938,     0.38038,     0.38138,     0.38238,     0.38338,\n",
              "           0.38438,     0.38539,     0.38639,     0.38739,     0.38839,     0.38939,     0.39039,     0.39139,     0.39239,     0.39339,     0.39439,      0.3954,      0.3964,      0.3974,      0.3984,      0.3994,      0.4004,      0.4014,      0.4024,      0.4034,      0.4044,     0.40541,     0.40641,     0.40741,\n",
              "           0.40841,     0.40941,     0.41041,     0.41141,     0.41241,     0.41341,     0.41441,     0.41542,     0.41642,     0.41742,     0.41842,     0.41942,     0.42042,     0.42142,     0.42242,     0.42342,     0.42442,     0.42543,     0.42643,     0.42743,     0.42843,     0.42943,     0.43043,     0.43143,\n",
              "           0.43243,     0.43343,     0.43443,     0.43544,     0.43644,     0.43744,     0.43844,     0.43944,     0.44044,     0.44144,     0.44244,     0.44344,     0.44444,     0.44545,     0.44645,     0.44745,     0.44845,     0.44945,     0.45045,     0.45145,     0.45245,     0.45345,     0.45445,     0.45546,\n",
              "           0.45646,     0.45746,     0.45846,     0.45946,     0.46046,     0.46146,     0.46246,     0.46346,     0.46446,     0.46547,     0.46647,     0.46747,     0.46847,     0.46947,     0.47047,     0.47147,     0.47247,     0.47347,     0.47447,     0.47548,     0.47648,     0.47748,     0.47848,     0.47948,\n",
              "           0.48048,     0.48148,     0.48248,     0.48348,     0.48448,     0.48549,     0.48649,     0.48749,     0.48849,     0.48949,     0.49049,     0.49149,     0.49249,     0.49349,     0.49449,      0.4955,      0.4965,      0.4975,      0.4985,      0.4995,      0.5005,      0.5015,      0.5025,      0.5035,\n",
              "            0.5045,     0.50551,     0.50651,     0.50751,     0.50851,     0.50951,     0.51051,     0.51151,     0.51251,     0.51351,     0.51451,     0.51552,     0.51652,     0.51752,     0.51852,     0.51952,     0.52052,     0.52152,     0.52252,     0.52352,     0.52452,     0.52553,     0.52653,     0.52753,\n",
              "           0.52853,     0.52953,     0.53053,     0.53153,     0.53253,     0.53353,     0.53453,     0.53554,     0.53654,     0.53754,     0.53854,     0.53954,     0.54054,     0.54154,     0.54254,     0.54354,     0.54454,     0.54555,     0.54655,     0.54755,     0.54855,     0.54955,     0.55055,     0.55155,\n",
              "           0.55255,     0.55355,     0.55455,     0.55556,     0.55656,     0.55756,     0.55856,     0.55956,     0.56056,     0.56156,     0.56256,     0.56356,     0.56456,     0.56557,     0.56657,     0.56757,     0.56857,     0.56957,     0.57057,     0.57157,     0.57257,     0.57357,     0.57457,     0.57558,\n",
              "           0.57658,     0.57758,     0.57858,     0.57958,     0.58058,     0.58158,     0.58258,     0.58358,     0.58458,     0.58559,     0.58659,     0.58759,     0.58859,     0.58959,     0.59059,     0.59159,     0.59259,     0.59359,     0.59459,      0.5956,      0.5966,      0.5976,      0.5986,      0.5996,\n",
              "            0.6006,      0.6016,      0.6026,      0.6036,      0.6046,     0.60561,     0.60661,     0.60761,     0.60861,     0.60961,     0.61061,     0.61161,     0.61261,     0.61361,     0.61461,     0.61562,     0.61662,     0.61762,     0.61862,     0.61962,     0.62062,     0.62162,     0.62262,     0.62362,\n",
              "           0.62462,     0.62563,     0.62663,     0.62763,     0.62863,     0.62963,     0.63063,     0.63163,     0.63263,     0.63363,     0.63463,     0.63564,     0.63664,     0.63764,     0.63864,     0.63964,     0.64064,     0.64164,     0.64264,     0.64364,     0.64464,     0.64565,     0.64665,     0.64765,\n",
              "           0.64865,     0.64965,     0.65065,     0.65165,     0.65265,     0.65365,     0.65465,     0.65566,     0.65666,     0.65766,     0.65866,     0.65966,     0.66066,     0.66166,     0.66266,     0.66366,     0.66466,     0.66567,     0.66667,     0.66767,     0.66867,     0.66967,     0.67067,     0.67167,\n",
              "           0.67267,     0.67367,     0.67467,     0.67568,     0.67668,     0.67768,     0.67868,     0.67968,     0.68068,     0.68168,     0.68268,     0.68368,     0.68468,     0.68569,     0.68669,     0.68769,     0.68869,     0.68969,     0.69069,     0.69169,     0.69269,     0.69369,     0.69469,      0.6957,\n",
              "            0.6967,      0.6977,      0.6987,      0.6997,      0.7007,      0.7017,      0.7027,      0.7037,      0.7047,     0.70571,     0.70671,     0.70771,     0.70871,     0.70971,     0.71071,     0.71171,     0.71271,     0.71371,     0.71471,     0.71572,     0.71672,     0.71772,     0.71872,     0.71972,\n",
              "           0.72072,     0.72172,     0.72272,     0.72372,     0.72472,     0.72573,     0.72673,     0.72773,     0.72873,     0.72973,     0.73073,     0.73173,     0.73273,     0.73373,     0.73473,     0.73574,     0.73674,     0.73774,     0.73874,     0.73974,     0.74074,     0.74174,     0.74274,     0.74374,\n",
              "           0.74474,     0.74575,     0.74675,     0.74775,     0.74875,     0.74975,     0.75075,     0.75175,     0.75275,     0.75375,     0.75475,     0.75576,     0.75676,     0.75776,     0.75876,     0.75976,     0.76076,     0.76176,     0.76276,     0.76376,     0.76476,     0.76577,     0.76677,     0.76777,\n",
              "           0.76877,     0.76977,     0.77077,     0.77177,     0.77277,     0.77377,     0.77477,     0.77578,     0.77678,     0.77778,     0.77878,     0.77978,     0.78078,     0.78178,     0.78278,     0.78378,     0.78478,     0.78579,     0.78679,     0.78779,     0.78879,     0.78979,     0.79079,     0.79179,\n",
              "           0.79279,     0.79379,     0.79479,      0.7958,      0.7968,      0.7978,      0.7988,      0.7998,      0.8008,      0.8018,      0.8028,      0.8038,      0.8048,     0.80581,     0.80681,     0.80781,     0.80881,     0.80981,     0.81081,     0.81181,     0.81281,     0.81381,     0.81481,     0.81582,\n",
              "           0.81682,     0.81782,     0.81882,     0.81982,     0.82082,     0.82182,     0.82282,     0.82382,     0.82482,     0.82583,     0.82683,     0.82783,     0.82883,     0.82983,     0.83083,     0.83183,     0.83283,     0.83383,     0.83483,     0.83584,     0.83684,     0.83784,     0.83884,     0.83984,\n",
              "           0.84084,     0.84184,     0.84284,     0.84384,     0.84484,     0.84585,     0.84685,     0.84785,     0.84885,     0.84985,     0.85085,     0.85185,     0.85285,     0.85385,     0.85485,     0.85586,     0.85686,     0.85786,     0.85886,     0.85986,     0.86086,     0.86186,     0.86286,     0.86386,\n",
              "           0.86486,     0.86587,     0.86687,     0.86787,     0.86887,     0.86987,     0.87087,     0.87187,     0.87287,     0.87387,     0.87487,     0.87588,     0.87688,     0.87788,     0.87888,     0.87988,     0.88088,     0.88188,     0.88288,     0.88388,     0.88488,     0.88589,     0.88689,     0.88789,\n",
              "           0.88889,     0.88989,     0.89089,     0.89189,     0.89289,     0.89389,     0.89489,      0.8959,      0.8969,      0.8979,      0.8989,      0.8999,      0.9009,      0.9019,      0.9029,      0.9039,      0.9049,     0.90591,     0.90691,     0.90791,     0.90891,     0.90991,     0.91091,     0.91191,\n",
              "           0.91291,     0.91391,     0.91491,     0.91592,     0.91692,     0.91792,     0.91892,     0.91992,     0.92092,     0.92192,     0.92292,     0.92392,     0.92492,     0.92593,     0.92693,     0.92793,     0.92893,     0.92993,     0.93093,     0.93193,     0.93293,     0.93393,     0.93493,     0.93594,\n",
              "           0.93694,     0.93794,     0.93894,     0.93994,     0.94094,     0.94194,     0.94294,     0.94394,     0.94494,     0.94595,     0.94695,     0.94795,     0.94895,     0.94995,     0.95095,     0.95195,     0.95295,     0.95395,     0.95495,     0.95596,     0.95696,     0.95796,     0.95896,     0.95996,\n",
              "           0.96096,     0.96196,     0.96296,     0.96396,     0.96496,     0.96597,     0.96697,     0.96797,     0.96897,     0.96997,     0.97097,     0.97197,     0.97297,     0.97397,     0.97497,     0.97598,     0.97698,     0.97798,     0.97898,     0.97998,     0.98098,     0.98198,     0.98298,     0.98398,\n",
              "           0.98498,     0.98599,     0.98699,     0.98799,     0.98899,     0.98999,     0.99099,     0.99199,     0.99299,     0.99399,     0.99499,       0.996,       0.997,       0.998,       0.999,           1]), array([[    0.84286,     0.84286,     0.84286,     0.82857,     0.82857,     0.82857,     0.81429,     0.81429,         0.8,     0.78571,     0.78571,     0.78571,     0.78571,     0.78571,     0.78571,     0.78571,     0.78571,      0.7831,     0.77143,     0.77143,      0.7639,     0.75714,     0.75714,\n",
              "            0.75714,     0.75714,     0.75714,     0.75714,     0.75714,     0.75714,     0.75714,     0.75714,     0.75714,     0.75714,     0.75714,     0.75706,     0.74563,     0.74286,     0.72857,     0.72857,     0.72857,     0.72857,     0.72857,     0.72857,     0.72857,     0.72857,     0.72857,\n",
              "            0.72857,     0.72857,     0.72857,     0.72857,     0.72857,     0.72857,     0.72857,     0.72857,     0.72857,     0.72857,     0.72857,     0.72857,     0.72857,     0.72857,     0.71429,     0.71429,     0.71429,     0.71429,     0.71429,     0.71429,     0.71429,     0.71429,     0.70701,\n",
              "            0.69449,     0.68571,     0.68571,     0.68571,     0.68571,     0.68327,     0.67949,     0.67571,     0.67193,     0.66972,     0.66775,     0.66578,     0.66381,     0.66184,     0.65988,     0.65791,     0.65714,     0.65714,     0.65697,     0.65594,     0.65492,      0.6539,     0.65287,\n",
              "            0.65185,     0.65083,      0.6498,     0.64878,     0.64776,     0.64673,     0.64571,     0.64469,     0.64367,     0.64286,     0.64286,     0.64286,     0.64286,     0.64286,     0.64286,     0.64286,     0.64286,     0.64286,     0.64286,     0.64286,     0.64279,      0.6422,      0.6416,\n",
              "            0.64101,     0.64042,     0.63982,     0.63923,     0.63864,     0.63805,     0.63745,     0.63686,     0.63627,     0.63567,     0.63508,     0.63449,     0.63389,      0.6333,     0.63271,     0.63211,     0.63152,     0.63093,     0.63033,     0.62974,     0.62915,     0.62857,     0.62857,\n",
              "            0.62857,     0.62857,     0.62857,     0.62857,     0.62857,     0.62857,     0.62857,     0.62857,     0.62727,     0.62596,     0.62466,     0.62336,     0.62206,     0.62076,     0.61946,     0.61815,     0.61685,     0.61555,     0.61429,     0.61429,     0.61429,     0.61429,     0.61429,\n",
              "            0.61429,     0.61429,     0.61429,     0.61429,     0.61429,     0.61429,     0.61429,     0.61429,     0.61429,     0.61429,     0.61429,     0.61429,     0.61429,     0.61429,     0.61429,     0.61429,     0.61429,     0.61429,     0.61429,     0.61429,     0.61429,     0.61429,     0.61429,\n",
              "            0.61429,     0.61429,     0.61429,     0.61429,     0.61429,     0.61429,     0.61429,     0.61429,     0.61429,     0.61429,     0.61429,     0.61429,     0.61429,     0.61429,     0.61429,     0.61429,     0.61429,     0.61429,     0.61429,     0.61429,     0.61429,     0.61429,     0.61429,\n",
              "            0.61429,     0.61429,     0.61429,     0.61429,     0.61429,     0.61429,     0.61429,     0.61429,     0.61429,     0.61429,     0.61429,     0.61429,     0.61429,     0.61429,     0.61429,     0.61429,     0.61429,     0.61429,     0.61429,     0.61429,     0.61429,     0.61429,     0.61429,\n",
              "            0.61429,     0.61429,     0.61429,     0.61429,     0.61429,     0.61429,     0.61429,     0.61429,     0.61429,     0.61429,     0.61429,     0.61429,     0.61429,     0.61429,     0.61429,     0.61429,     0.61429,     0.61429,     0.61429,     0.61429,     0.61429,     0.61429,     0.61429,\n",
              "            0.61429,     0.61429,     0.61429,     0.61429,     0.61429,     0.61429,     0.61429,     0.61429,     0.61429,     0.61429,     0.61429,     0.61429,     0.61429,     0.61429,     0.61429,     0.61429,     0.61429,     0.61429,     0.61429,     0.61429,     0.61429,     0.61429,     0.61429,\n",
              "            0.61429,     0.61429,     0.61429,     0.61429,     0.61429,     0.61429,     0.61429,     0.61429,     0.61429,     0.61429,     0.61429,     0.61429,     0.61429,     0.61429,     0.61429,     0.61429,     0.61429,     0.61429,     0.61429,     0.61429,     0.61429,     0.61429,     0.61429,\n",
              "            0.61429,     0.61429,     0.61429,     0.61429,     0.61429,     0.61429,     0.61429,     0.61429,     0.61429,     0.61429,     0.61429,     0.61429,     0.61429,     0.61429,     0.61429,     0.61429,     0.61429,     0.61429,     0.61429,     0.61429,     0.61429,     0.61429,     0.61429,\n",
              "            0.61429,     0.61429,     0.61429,     0.61429,     0.61429,     0.61429,     0.61429,     0.61429,     0.61429,     0.61429,     0.61429,     0.61429,     0.61429,     0.61429,     0.61429,     0.61429,     0.61429,     0.61429,     0.61429,     0.61429,     0.61429,     0.61429,     0.61429,\n",
              "            0.61429,     0.61429,     0.61429,     0.61429,     0.61429,     0.61429,     0.61429,     0.61429,     0.61429,     0.61429,     0.61429,     0.61429,     0.61429,     0.61429,     0.61429,     0.61411,     0.61393,     0.61375,     0.61358,      0.6134,     0.61322,     0.61304,     0.61286,\n",
              "            0.61269,     0.61251,     0.61233,     0.61215,     0.61197,      0.6118,     0.61162,     0.61144,     0.61126,     0.61108,     0.61091,     0.61073,     0.61055,     0.61037,     0.61019,     0.61002,     0.60984,     0.60966,     0.60948,      0.6093,     0.60913,     0.60895,     0.60877,\n",
              "            0.60859,     0.60841,     0.60824,     0.60806,     0.60788,      0.6077,     0.60752,     0.60735,     0.60717,     0.60699,     0.60681,     0.60663,     0.60646,     0.60628,      0.6061,     0.60592,     0.60574,     0.60556,     0.60539,     0.60521,     0.60503,     0.60485,     0.60467,\n",
              "             0.6045,     0.60432,     0.60414,     0.60396,     0.60378,     0.60361,     0.60343,     0.60325,     0.60307,     0.60289,     0.60272,     0.60254,     0.60236,     0.60218,       0.602,     0.60183,     0.60165,     0.60147,     0.60129,     0.60111,     0.60094,     0.60076,     0.60058,\n",
              "             0.6004,     0.60022,     0.60005,     0.58913,     0.58543,     0.58506,      0.5847,     0.58433,     0.58396,     0.58359,     0.58322,     0.58285,     0.58248,     0.58212,     0.58175,     0.58138,     0.58101,     0.58064,     0.58027,     0.57991,     0.57954,     0.57917,      0.5788,\n",
              "            0.57843,     0.57806,      0.5777,     0.57733,     0.57696,     0.57659,     0.57622,     0.57585,     0.57549,     0.57512,     0.57475,     0.57438,     0.57401,     0.57364,     0.57328,     0.57291,     0.57254,     0.57217,      0.5718,     0.57143,      0.5713,     0.57118,     0.57105,\n",
              "            0.57092,     0.57079,     0.57067,     0.57054,     0.57041,     0.57028,     0.57016,     0.57003,      0.5699,     0.56977,     0.56965,     0.56952,     0.56939,     0.56927,     0.56914,     0.56901,     0.56888,     0.56876,     0.56863,      0.5685,     0.56837,     0.56825,     0.56812,\n",
              "            0.56799,     0.56786,     0.56774,     0.56761,     0.56748,     0.56736,     0.56723,      0.5671,     0.56697,     0.56685,     0.56672,     0.56659,     0.56646,     0.56634,     0.56621,     0.56608,     0.56595,     0.56583,      0.5657,     0.56557,     0.56545,     0.56532,     0.56519,\n",
              "            0.56506,     0.56494,     0.56481,     0.56468,     0.56455,     0.56443,      0.5643,     0.56417,     0.56404,     0.56392,     0.56379,     0.56366,     0.56354,     0.56341,     0.56328,     0.56315,     0.56303,      0.5629,     0.56277,     0.56264,     0.56252,     0.56239,     0.56226,\n",
              "            0.56213,     0.56201,     0.56188,     0.56175,     0.56163,      0.5615,     0.56137,     0.56124,     0.56112,     0.56099,     0.56086,     0.56073,     0.56061,     0.56048,     0.56035,     0.56022,      0.5601,     0.55997,     0.55984,     0.55972,     0.55959,     0.55946,     0.55933,\n",
              "            0.55921,     0.55908,     0.55895,     0.55882,      0.5587,     0.55857,     0.55844,     0.55831,     0.55819,     0.55806,     0.55793,     0.55781,     0.55768,     0.55755,     0.55742,      0.5573,     0.55717,     0.55132,       0.544,     0.53933,     0.53514,     0.53096,     0.52826,\n",
              "            0.52753,     0.52679,     0.52606,     0.52533,      0.5246,     0.52386,     0.52313,      0.5224,     0.52167,     0.52094,      0.5202,     0.51947,     0.51874,     0.51801,     0.51727,     0.51654,     0.51581,     0.51508,     0.51435,     0.51237,     0.51027,     0.50818,     0.50609,\n",
              "              0.504,     0.50191,     0.49996,      0.4995,     0.49904,     0.49859,     0.49813,     0.49767,     0.49721,     0.49676,      0.4963,     0.49584,     0.49538,     0.49493,     0.49447,     0.49401,     0.49355,      0.4931,     0.49264,     0.49218,     0.49172,     0.49127,     0.49081,\n",
              "            0.49035,     0.48989,     0.48943,     0.48898,     0.48852,     0.48806,      0.4876,     0.48715,     0.48669,     0.48623,     0.48577,      0.4847,     0.48352,     0.48235,     0.48118,     0.48001,     0.47884,     0.47767,      0.4765,     0.47532,     0.47415,     0.47298,     0.47181,\n",
              "            0.46649,     0.45917,     0.45412,     0.44993,     0.44575,     0.44156,     0.43738,      0.4332,     0.42901,     0.42483,     0.42065,     0.41646,      0.4096,     0.39993,     0.39575,     0.39156,     0.38738,     0.38131,     0.37398,     0.36871,     0.36452,     0.36034,     0.35645,\n",
              "            0.35352,     0.35059,     0.34766,     0.34474,     0.34136,     0.33717,     0.33299,     0.32881,     0.32396,     0.31908,     0.31416,     0.30684,     0.29936,     0.28959,     0.25538,     0.25245,     0.24952,     0.24659,     0.24366,     0.23983,     0.23564,     0.23146,     0.19697,\n",
              "            0.18721,      0.1673,     0.16241,     0.15753,     0.15041,     0.14309,      0.1334,      0.1261,     0.12122,     0.11634,     0.11298,     0.11073,     0.10848,     0.10622,     0.10397,     0.10172,    0.097683,    0.087921,    0.081937,    0.077056,    0.072175,    0.065226,    0.057904,\n",
              "           0.055893,    0.054499,    0.053104,     0.05171,    0.050315,     0.04892,    0.047526,    0.046131,    0.044737,    0.043342,     0.04215,    0.041065,     0.03998,    0.038896,    0.037811,    0.036726,    0.035641,    0.034557,    0.033472,    0.032387,    0.031303,    0.030218,    0.029133,\n",
              "           0.028085,    0.027075,    0.026065,    0.025055,    0.024045,    0.023035,    0.022025,    0.021016,    0.020006,    0.018996,    0.017986,    0.016976,    0.015966,    0.014956,           0,           0,           0,           0,           0,           0,           0,           0,           0,\n",
              "                  0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,\n",
              "                  0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,\n",
              "                  0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,\n",
              "                  0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,\n",
              "                  0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,\n",
              "                  0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,\n",
              "                  0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,\n",
              "                  0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,\n",
              "                  0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,\n",
              "                  0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0]]), 'Confidence', 'Recall']]\n",
              "fitness: 0.5271215430346166\n",
              "keys: ['metrics/precision(B)', 'metrics/recall(B)', 'metrics/mAP50(B)', 'metrics/mAP50-95(B)']\n",
              "maps: array([    0.49796])\n",
              "names: {0: 'football'}\n",
              "plot: True\n",
              "results_dict: {'metrics/precision(B)': 0.9973035063287771, 'metrics/recall(B)': 0.6142857142857143, 'metrics/mAP50(B)': 0.7896057722872329, 'metrics/mAP50-95(B)': 0.49795662867321466, 'fitness': 0.5271215430346166}\n",
              "save_dir: PosixPath('runs/detect/train5')\n",
              "speed: {'preprocess': 1.8893727889427772, 'inference': 21.332187835986797, 'loss': 0.0012571995074932391, 'postprocess': 4.1371528918926535}\n",
              "task: 'detect'"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sahi import AutoDetectionModel # import from sahi.models instead of sahi.model\n",
        "from sahi.predict import get_sliced_prediction\n",
        "from sahi.utils.cv import read_image\n",
        "import cv2"
      ],
      "metadata": {
        "id": "2Qq0SQHV9jmC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sahi.predict import get_sliced_prediction\n",
        "import cv2"
      ],
      "metadata": {
        "id": "PPTI5wivOmsy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = \"/content/best.pt\"  # Replace with your YOLOv8 model\n",
        "\n",
        "detection_model = AutoDetectionModel.from_pretrained(\n",
        "    model_type=\"yolov8\",\n",
        "    model_path=model_path,\n",
        "    confidence_threshold=0.3,  # Adjust as needed\n",
        "    device=\"cuda\"  # Use \"cpu\" if no GPU is available\n",
        ")"
      ],
      "metadata": {
        "id": "XcnDnnAROufC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Input video path\n",
        "video_path = \"/content/08fd33_4.mp4\"\n",
        "output_path = \"new_video.mp4\"\n",
        "\n",
        "# Open video file\n",
        "cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "# Get video properties\n",
        "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
        "\n",
        "# Define video writer for saving output\n",
        "fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
        "out = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n",
        "\n",
        "while cap.isOpened():\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    # Get predictions with SAHI\n",
        "    result = get_sliced_prediction(\n",
        "        image=frame,\n",
        "        detection_model=detection_model,\n",
        "        slice_height=960,  # Adjust slice height\n",
        "        slice_width=540,   # Adjust slice width\n",
        "        overlap_height_ratio=0.2,  # Adjust overlap as needed\n",
        "        overlap_width_ratio=0.2,\n",
        "    )\n",
        "\n",
        "    # Draw predictions on the frame\n",
        "    for obj in result.object_prediction_list:\n",
        "        bbox = obj.bbox\n",
        "        class_name = obj.category.name\n",
        "        # Access the actual confidence value using .value\n",
        "        confidence = obj.score.value\n",
        "\n",
        "        # Draw bounding box and label\n",
        "        x1, y1, x2, y2 = map(int, [bbox.minx, bbox.miny, bbox.maxx, bbox.maxy])\n",
        "        cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
        "        label = f\"{class_name}: {confidence:.2f}\" # Now formatting works as confidence is a float\n",
        "        cv2.putText(frame, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
        "\n",
        "    # Write the frame to the output video\n",
        "    out.write(frame)\n",
        "\n",
        "# Release resources\n",
        "cap.release()\n",
        "out.release()\n",
        "cv2.destroyAllWindows()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O40ScBgKO3tz",
        "outputId": "f9266c76-a33d-4983-eaad-6e9ff9046acf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n",
            "Performing prediction on 10 slices.\n"
          ]
        }
      ]
    }
  ]
}
