{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MODEL V8**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "import supervision as sv\n",
    "#NEED TO DOWNLOAD AND UPLOAD VIDEO IF YOU USING GG COLAB\n",
    "# Define the video path\n",
    "MARKET_SQUARE_VIDEO_PATH = \"/content/market-square.mp4\"\n",
    "\n",
    "# Initialize the YOLOv8 model\n",
    "model = YOLO(\"yolov8n.pt\")\n",
    "\n",
    "# Open the video file\n",
    "cap = cv2.VideoCapture(MARKET_SQUARE_VIDEO_PATH)\n",
    "\n",
    "# Verify the output directory and permissions\n",
    "output_dir = \"/content\"\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "if not os.access(output_dir, os.W_OK):\n",
    "    raise PermissionError(f\"Write permission denied for the directory {output_dir}\")\n",
    "\n",
    "# Define the output video path\n",
    "output_path = os.path.join(output_dir, \"object_counting_output_v8_3.mp4\")\n",
    "\n",
    "assert cap.isOpened(), \"Error reading video file\"\n",
    "\n",
    "# Get video properties\n",
    "w, h, fps = (int(cap.get(x)) for x in (cv2.CAP_PROP_FRAME_WIDTH, cv2.CAP_PROP_FRAME_HEIGHT, cv2.CAP_PROP_FPS))\n",
    "\n",
    "# Initialize VideoWriter with a successful FourCC code\n",
    "fourcc_code = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "video_writer = cv2.VideoWriter(output_path, fourcc_code, fps, (w, h))\n",
    "\n",
    "if not video_writer.isOpened():\n",
    "    raise IOError(f\"Error initializing video writer with path {output_path}\")\n",
    "\n",
    "# Custom function to filter only human detections\n",
    "def filter_human_detections(detections):\n",
    "    human_class_id = 0\n",
    "    mask = detections.class_id == human_class_id\n",
    "    filtered_detections = sv.Detections(\n",
    "        xyxy=detections.xyxy[mask],\n",
    "        confidence=detections.confidence[mask],\n",
    "        class_id=detections.class_id[mask]\n",
    "    )\n",
    "    return filtered_detections\n",
    "\n",
    "# Process video frames\n",
    "box_annotator = sv.BoxAnnotator(thickness=4, text_thickness=4, text_scale=2)\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Video frame is empty or video processing has been successfully completed.\")\n",
    "        break\n",
    "\n",
    "    # Perform object detection\n",
    "    results = model(frame, imgsz=1280)\n",
    "\n",
    "    # Extract bounding boxes, confidences, and class IDs\n",
    "    boxes = results[0].boxes.xyxy.cpu().numpy()\n",
    "    confidences = results[0].boxes.conf.cpu().numpy()\n",
    "    class_ids = results[0].boxes.cls.cpu().numpy().astype(int)\n",
    "\n",
    "    # Create detections object\n",
    "    detections = sv.Detections(\n",
    "        xyxy=boxes,\n",
    "        confidence=confidences,\n",
    "        class_id=class_ids\n",
    "    )\n",
    "\n",
    "    # Filter only human detections\n",
    "    human_detections = filter_human_detections(detections)\n",
    "\n",
    "    # Annotate the frame\n",
    "    annotated_frame = box_annotator.annotate(scene=frame, detections=human_detections)\n",
    "\n",
    "    # Write the processed frame to the output video\n",
    "    video_writer.write(annotated_frame)\n",
    "\n",
    "cap.release()\n",
    "video_writer.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "print(f\"Processed video saved to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
