{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82d99c1e",
   "metadata": {},
   "source": [
    "## Nvidia GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "528cefe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Dec  9 02:04:14 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 552.22                 Driver Version: 552.22         CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                     TCC/WDDM  | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3060 ...  WDDM  |   00000000:01:00.0 Off |                  N/A |\n",
      "| N/A   57C    P8              8W /  115W |      52MiB /   6144MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A       648    C+G   C:\\Program Files\\Tencent\\QQNT\\QQ.exe        N/A      |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f0d6d4",
   "metadata": {},
   "source": [
    "## Frameworks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a009c882",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2020 NVIDIA Corporation\n",
      "Built on Mon_Nov_30_19:15:10_Pacific_Standard_Time_2020\n",
      "Cuda compilation tools, release 11.2, V11.2.67\n",
      "Build cuda_11.2.r11.2/compiler.29373293_0\n",
      "torch:  1.4 ; cuda:  cu92\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "!nvcc --version\n",
    "TORCH_VERSION = \".\".join(torch.__version__.split(\".\")[:2])\n",
    "CUDA_VERSION = torch.__version__.split(\"+\")[-1]\n",
    "print(\"torch: \", TORCH_VERSION, \"; cuda: \", CUDA_VERSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "339d2792",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db7cf467",
   "metadata": {},
   "outputs": [],
   "source": [
    "path: ../\n",
    "\n",
    "train: train\n",
    "val: valid\n",
    "test: test\n",
    "\n",
    "nc: 1\n",
    "names: ['face']\n",
    "\n",
    "# roboflow:\n",
    "#  workspace: uniform-fhonp\n",
    "#  project: face-detection-quvrj\n",
    "#  version: 1\n",
    "#  license: CC BY 4.0\n",
    "#  url: https://universe.roboflow.com/uniform-fhonp/face-detection-quvrj/dataset/1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1b8b07",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b6a2a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model = YOLO(\"yolov8n.pt\")\n",
    "    model.train(data=\"../FaceData/data.yaml\", epochs=100)\n",
    "    model.val()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b8d047b",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4390519",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model = YOLO(\"./face_detector.pt\")\n",
    "    model.val()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff79f36",
   "metadata": {},
   "source": [
    "## Image Prediction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1269fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding:utf-8\n",
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "\n",
    "def validate_file_path(file_path, base_dir=\".\"):\n",
    "    \"\"\"\n",
    "    Validate if a file path is within a specific base directory.\n",
    "    Prevents directory traversal attacks.\n",
    "    \"\"\"\n",
    "    abs_path = os.path.abspath(file_path)\n",
    "    base_dir = os.path.abspath(base_dir)\n",
    "\n",
    "    if not abs_path.startswith(base_dir):\n",
    "        raise ValueError(f\"Invalid file path: {file_path}\")\n",
    "\n",
    "    return abs_path\n",
    "\n",
    "\n",
    "def img_cvread(img_path):\n",
    "    \"\"\"Read the image and return the image array\"\"\"\n",
    "    return cv2.imread(img_path)\n",
    "\n",
    "\n",
    "def face_detect(cv_img, face_model):\n",
    "    \"\"\"Perform face detection and return detected face images and their locations\"\"\"\n",
    "    results = face_model(cv_img)\n",
    "    faces = []\n",
    "    locations = []\n",
    "    for result in results:\n",
    "        for bbox in result.boxes:\n",
    "            x1, y1, x2, y2 = map(int, bbox.xyxy[0].cpu().numpy())\n",
    "            face = cv_img[y1:y2, x1:x2]\n",
    "            faces.append(face)\n",
    "            locations.append((x1, y1, x2, y2))\n",
    "    return cv_img, faces, locations\n",
    "\n",
    "\n",
    "def adjust_parameters(width, height):\n",
    "    \"\"\"Adjust parameters based on image size\"\"\"\n",
    "    base_width = 640\n",
    "    scale = min(width / base_width, height / base_width)\n",
    "    box_thickness = int(10 * scale)\n",
    "    return box_thickness\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    base_dir = \".\"  # Define a base directory to validate file paths\n",
    "\n",
    "    img_path = \"\"       # Input image file path\n",
    "    output_path = \"\"    # Output image file path\n",
    "\n",
    "    # Validate file paths\n",
    "    try:\n",
    "        img_path = validate_file_path(img_path, base_dir)\n",
    "        output_path = validate_file_path(output_path, base_dir)\n",
    "    except ValueError as e:\n",
    "        print(e)\n",
    "        exit()\n",
    "\n",
    "    # Path to the face detection model\n",
    "    face_model_path = 'face_detector.pt'\n",
    "\n",
    "    # Load the face detection model\n",
    "    face_model = YOLO(face_model_path, task='detect')\n",
    "\n",
    "    cv_img = img_cvread(img_path)\n",
    "    if cv_img is None:\n",
    "        print(f\"Error: Could not read image from path: {img_path}\")\n",
    "        exit()\n",
    "\n",
    "    height, width = cv_img.shape[:2]\n",
    "\n",
    "    # Adjust parameters\n",
    "    box_thickness = adjust_parameters(width, height)\n",
    "\n",
    "    # Perform face detection\n",
    "    face_cvimg, faces, locations = face_detect(cv_img, face_model)\n",
    "\n",
    "    if faces:\n",
    "        for i in range(len(faces)):\n",
    "            left, top, right, bottom = locations[i]\n",
    "            # Draw rectangle around the detected face\n",
    "            face_cvimg = cv2.rectangle(face_cvimg, (left, top), (right, bottom), (50, 50, 250), box_thickness)\n",
    "\n",
    "    # Save the predicted image\n",
    "    cv2.imwrite(output_path, face_cvimg)\n",
    "\n",
    "    # Display the predicted image\n",
    "    cv2.imshow('yolov8_detections', face_cvimg)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c0a273",
   "metadata": {},
   "source": [
    "## Video Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a96fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding:utf-8\n",
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "import os\n",
    "from moviepy.editor import VideoFileClip, AudioFileClip\n",
    "\n",
    "\n",
    "def validate_file_path(file_path, base_dir=\".\"):\n",
    "    \"\"\"\n",
    "    Validate if a file path is within a specific base directory.\n",
    "    Prevents directory traversal attacks.\n",
    "    \"\"\"\n",
    "    abs_path = os.path.abspath(file_path)\n",
    "    base_dir = os.path.abspath(base_dir)\n",
    "\n",
    "    if not abs_path.startswith(base_dir):\n",
    "        raise ValueError(f\"Invalid file path: {file_path}\")\n",
    "\n",
    "    return abs_path\n",
    "\n",
    "\n",
    "def face_detect(cv_img, face_model):\n",
    "    \"\"\"Perform face detection and return detected face images and their locations\"\"\"\n",
    "    results = face_model(cv_img)\n",
    "    faces = []\n",
    "    locations = []\n",
    "    for result in results:\n",
    "        for bbox in result.boxes:\n",
    "            x1, y1, x2, y2 = map(int, bbox.xyxy[0].cpu().numpy())\n",
    "            face = cv_img[y1:y2, x1:x2]\n",
    "            faces.append(face)\n",
    "            locations.append((x1, y1, x2, y2))\n",
    "    return cv_img, locations\n",
    "\n",
    "\n",
    "def process_frame(frame, face_model):\n",
    "    \"\"\"Process video frame for face detection\"\"\"\n",
    "    face_cvimg, locations = face_detect(frame, face_model)\n",
    "\n",
    "    if locations:\n",
    "        for (left, top, right, bottom) in locations:\n",
    "            # Draw rectangle around detected faces\n",
    "            face_cvimg = cv2.rectangle(face_cvimg, (left, top), (right, bottom), (50, 50, 250), 2)\n",
    "\n",
    "    return face_cvimg\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    base_dir = \".\"  # Define a base directory to validate file paths\n",
    "\n",
    "    video_path = \"\"     # Input video file path\n",
    "    output_path = \"\"    # Output video file path\n",
    "\n",
    "    # Validate file paths\n",
    "    try:\n",
    "        video_path = validate_file_path(video_path, base_dir)\n",
    "        output_path = validate_file_path(output_path, base_dir)\n",
    "    except ValueError as e:\n",
    "        print(e)\n",
    "        exit()\n",
    "\n",
    "    # Path to the face detection model\n",
    "    face_model_path = 'face_detector.pt'\n",
    "\n",
    "    # Load the face detection model\n",
    "    face_model = YOLO(face_model_path)\n",
    "\n",
    "    # Open the video file\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(f\"Error: Could not open video file: {video_path}\")\n",
    "        exit()\n",
    "\n",
    "    # Get video properties\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "    # Open video writer\n",
    "    out = cv2.VideoWriter('temp_video.mp4', cv2.VideoWriter_fourcc(*'mp4v'), fps, (width, height))\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Process each frame\n",
    "        processed_frame = process_frame(frame, face_model)\n",
    "\n",
    "        # Display the result\n",
    "        cv2.imshow('Face Detection', processed_frame)\n",
    "\n",
    "        # Write the processed frame\n",
    "        out.write(processed_frame)\n",
    "\n",
    "        # Exit on 'q' key press\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "    # Combine processed video with original audio\n",
    "    video_clip = VideoFileClip(\"temp_video.mp4\")\n",
    "    audio_clip = AudioFileClip(video_path)\n",
    "    final_clip = video_clip.set_audio(audio_clip)\n",
    "    final_clip.write_videofile(output_path, codec='libx264')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e25b59",
   "metadata": {},
   "source": [
    "##  Real-time Prediction from Camera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6043b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding:utf-8\n",
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "\n",
    "\n",
    "def face_detect(cv_img, face_model):\n",
    "    \"\"\"Perform face detection and return detected face images and their locations\"\"\"\n",
    "    results = face_model(cv_img)\n",
    "    faces = []\n",
    "    locations = []\n",
    "    for result in results:\n",
    "        for bbox in result.boxes:\n",
    "            x1, y1, x2, y2 = map(int, bbox.xyxy[0].cpu().numpy())\n",
    "            face = cv_img[y1:y2, x1:x2]\n",
    "            faces.append(face)\n",
    "            locations.append((x1, y1, x2, y2))\n",
    "    return cv_img, locations\n",
    "\n",
    "\n",
    "def adjust_parameters(width, height):\n",
    "    \"\"\"Adjust parameters based on image size\"\"\"\n",
    "    base_width = 640\n",
    "    scale = min(width / base_width, height / base_width)\n",
    "    box_thickness = int(10 * scale)\n",
    "    return box_thickness\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Path to the face detection model\n",
    "    face_model_path = 'face_detector.pt'\n",
    "\n",
    "    # Load the face detection model\n",
    "    face_model = YOLO(face_model_path, task='detect')\n",
    "\n",
    "    # Open the camera, 0 indicates the default camera\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Could not open camera.\")\n",
    "        exit()\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(\"Error: Failed to read frame.\")\n",
    "            break\n",
    "\n",
    "        height, width = frame.shape[:2]\n",
    "\n",
    "        # Adjust parameters\n",
    "        box_thickness = adjust_parameters(width, height)\n",
    "\n",
    "        # Perform face detection\n",
    "        face_cvimg, locations = face_detect(frame, face_model)\n",
    "\n",
    "        if locations:\n",
    "            for (left, top, right, bottom) in locations:\n",
    "                # Draw rectangle around detected faces\n",
    "                face_cvimg = cv2.rectangle(face_cvimg, (left, top), (right, bottom), (50, 50, 250), box_thickness)\n",
    "\n",
    "        # Display the frame with detection results\n",
    "        cv2.imshow('Face Detection System', face_cvimg)\n",
    "\n",
    "        # Exit the loop when 'q' key is pressed\n",
    "        if cv2.waitKey(1) == ord('q'):\n",
    "            break\n",
    "\n",
    "    # Release the camera resource\n",
    "    cap.release()\n",
    "    # Close all OpenCV windows\n",
    "    cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
