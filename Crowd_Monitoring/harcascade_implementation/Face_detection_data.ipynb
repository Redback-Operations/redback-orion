{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5558071a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install opencv-python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830d9931",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cmd\n",
    "where python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1fd9b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cmd\n",
    "pip install cmake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48126a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cmd\n",
    "python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8816c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cmd\n",
    "pip install \"dlib-19.24.1-cp311-cp311-win_amd64.whl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42562fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cmd \n",
    "pip install face-recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4adaca09-708f-43f4-a9c2-17a8b5f5df19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import dlib\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a33863be-bb61-46de-87e2-6efa9afa5637",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_yolo_model(weights_path, config_path):\n",
    "    net = cv2.dnn.readNet(weights_path, config_path)\n",
    "    layer_names = net.getLayerNames()\n",
    "    output_layers = [layer_names[i - 1] for i in net.getUnconnectedOutLayers()]\n",
    "    return net, output_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6f3e01ed-465e-4411-bd46-ff7b767d4f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_faces(img, net, output_layers, confidence_threshold=0.5):\n",
    "    height, width, channels = img.shape\n",
    "\n",
    "    # Prepare the image for the model\n",
    "    blob = cv2.dnn.blobFromImage(img, 0.00392, (416, 416), (0, 0, 0), True, crop=False)\n",
    "    net.setInput(blob)\n",
    "    outs = net.forward(output_layers)\n",
    "\n",
    "    boxes = []\n",
    "    confidences = []\n",
    "\n",
    "    # Processing the output\n",
    "    for out in outs:\n",
    "        for detection in out:\n",
    "            scores = detection[5:]\n",
    "            confidence = max(scores)\n",
    "            if confidence > confidence_threshold:  # Confidence threshold\n",
    "                center_x = int(detection[0] * width)\n",
    "                center_y = int(detection[1] * height)\n",
    "                w = int(detection[2] * width)\n",
    "                h = int(detection[3] * height)\n",
    "                x = int(center_x - w / 2)\n",
    "                y = int(center_y - h / 2)\n",
    "                boxes.append([x, y, w, h])\n",
    "                confidences.append(float(confidence))\n",
    "\n",
    "    indexes = cv2.dnn.NMSBoxes(boxes, confidences, 0.5, 0.4)\n",
    "\n",
    "    faces = []\n",
    "    selected_confidences = []\n",
    "    for i in indexes.flatten():\n",
    "        x, y, w, h = boxes[i]\n",
    "        face = img[y:y+h, x:x+w]\n",
    "        faces.append((x, y, w, h))\n",
    "        selected_confidences.append(confidences[i])  # Add the confidence for this face\n",
    "        cv2.rectangle(img, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "\n",
    "    return img, faces, selected_confidences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3a77d4c6-6b71-4a12-901b-421888c88109",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_face_characteristics(img, faces, shape_predictor, face_rec_model):\n",
    "    characteristics = []\n",
    "    for (x, y, w, h) in faces:\n",
    "        face = img[y:y+h, x:x+w]\n",
    "        rgb_face = cv2.cvtColor(face, cv2.COLOR_BGR2RGB)  # Convert to RGB\n",
    "        \n",
    "        # Predict face landmarks\n",
    "        shape = shape_predictor(rgb_face, dlib.rectangle(0, 0, face.shape[1], face.shape[0]))\n",
    "\n",
    "        # Draw landmarks and bounding boxes around them\n",
    "        for p in shape.parts():\n",
    "            cv2.circle(face, (p.x, p.y), 2, (0, 0, 255), -1)  # Red dots for landmarks\n",
    "            \n",
    "            # Draw bounding box around each landmark\n",
    "            landmark_bbox_size = 4  # Size of the bounding box around each landmark\n",
    "            cv2.rectangle(face, (p.x - landmark_bbox_size, p.y - landmark_bbox_size), \n",
    "                          (p.x + landmark_bbox_size, p.y + landmark_bbox_size), (255, 0, 0), 1)  # Blue rectangle around landmarks\n",
    "        \n",
    "        # Get the face embedding\n",
    "        face_descriptor = face_rec_model.compute_face_descriptor(rgb_face, shape)\n",
    "        \n",
    "        characteristics.append({\n",
    "            'bbox': (x, y, w, h),\n",
    "            'landmarks': [(p.x, p.y) for p in shape.parts()],\n",
    "            'embedding': np.array(face_descriptor)\n",
    "        })\n",
    "\n",
    "    return characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fc719eb4-bacb-4628-a503-c3dd54e1b4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_video(video_path, output_video_path, net, output_layers, shape_predictor, face_rec_model):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frame_rate = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "    frame_count = 0\n",
    "    result_data = []\n",
    "\n",
    "    # Prepare video writer\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "    out = cv2.VideoWriter(output_video_path, fourcc, frame_rate, \n",
    "                          (int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)), int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))))\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        if frame_count % frame_rate == 0:  # Process 1 frame per second\n",
    "            img_with_faces, faces, confidences = detect_faces(frame, net, output_layers, confidence_threshold=0.5)\n",
    "            characteristics = extract_face_characteristics(frame, faces, shape_predictor, face_rec_model)\n",
    "\n",
    "            # Store the results\n",
    "            frame_data = {\n",
    "                'frame_id': frame_count,\n",
    "                'face_count': len(faces),\n",
    "                'faces': []\n",
    "            }\n",
    "\n",
    "            for idx, face_data in enumerate(characteristics):\n",
    "                face_info = {\n",
    "                    'face_id': idx,\n",
    "                    'bbox': face_data['bbox'],\n",
    "                    'confidence': confidences[idx],  # Add confidence score\n",
    "                    'landmarks': face_data['landmarks'],\n",
    "                    'embedding': face_data['embedding'].tolist()  # Convert to list for JSON serialization\n",
    "                }\n",
    "                frame_data['faces'].append(face_info)\n",
    "\n",
    "            result_data.append(frame_data)\n",
    "            \n",
    "            # Print the JSON result of the current frame\n",
    "            print(json.dumps(frame_data, indent=4))\n",
    "            \n",
    "            # Write the frame with detected faces to the output video\n",
    "            out.write(img_with_faces)\n",
    "\n",
    "        frame_count += 1\n",
    "\n",
    "    cap.release()\n",
    "    out.release()\n",
    "\n",
    "    return result_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be95fc1c-5632-403f-a16b-7439d0f68c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    weights_path = \"yolov3-wider_16000.weights\"\n",
    "    config_path = \"yolov3-face.cfg\"\n",
    "    video_path = \"video_url\"\n",
    "    output_video_path = \"output_video_with_faces.avi\"\n",
    "    shape_predictor_path = \"shape_predictor_68_face_landmarks.dat\"\n",
    "    face_rec_model_path = \"dlib_face_recognition_resnet_model_v1.dat\"\n",
    "\n",
    "    net, output_layers = load_yolo_model(weights_path, config_path)\n",
    "    shape_predictor = dlib.shape_predictor(shape_predictor_path)\n",
    "    face_rec_model = dlib.face_recognition_model_v1(face_rec_model_path)\n",
    "\n",
    "    result_data = process_video(video_path, output_video_path, net, output_layers, shape_predictor, face_rec_model)\n",
    "\n",
    "    # Optionally, save the result data to a JSON file or print it\n",
    "    with open('result_data.json', 'w') as f:\n",
    "        json.dump(result_data, f, indent=4)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9498a7f9-e4cc-4282-bd56-51d16e8c67c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
